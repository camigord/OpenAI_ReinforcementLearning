{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Implementing Double DQN</h1> \n",
    "\n",
    "Implementation of [Double Q-learning](https://arxiv.org/abs/1509.06461) in TensorFlow.\n",
    "\n",
    "Double Q-learning was proposed as a way for alleviating the problem of overestimating the action values. It showed to perform significantly better in many atari games in comparison to thr standard DQN implementation.\n",
    "\n",
    "Implementing Double Q is actually quite simple once we have the standard DQN. I will use the previous DQN code and I will highlight the required modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters\n",
    "\n",
    "Configuration parameteres as presented in the [original](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'max_steps': 50000000,\n",
    "    'eval_freq': 50000,\n",
    "    'eval_steps': 50000,\n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    'pool_frame_size': 2,\n",
    "    \n",
    "    'memory_size': 1000000,       # Replay memory size\n",
    "    'batch_size': 32,             # Number of training cases ove which SGD update is computed\n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "    'learn_start': 50000,         # Replay start size\n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'ep_min': 0.1,                 # Final exploration\n",
    "    'ep_start': 1.0,               # Initial exploration\n",
    "    'ep_end_time': 1000000,        # Final exploration frame\n",
    "\n",
    "    'target_q_update_step': 10000, # Target network update frequency\n",
    "\n",
    "    # Train batch\n",
    "    'train_frequency': 4,          # Update frequency\n",
    "    \n",
    "    # Clip rewards\n",
    "    'min_reward': -1.0,\n",
    "    'max_reward': 1.0,\n",
    "\n",
    "    # How many times should the same action be taken\n",
    "    'action_repeat': 1,\n",
    "    \n",
    "    # Whether or not to render the environment \n",
    "    'display': False,\n",
    "    \n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    'log_performance_file': 'Logs/DoubleQ.txt'\n",
    "}\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the game environment\n",
    "\n",
    "Wrapper class arund the gym environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GameScreen:\n",
    "    def __init__(self, conf):\n",
    "        self.buffer = np.zeros([conf['pool_frame_size'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = screen\n",
    "        \n",
    "    def get(self):        \n",
    "        return np.amax(np.transpose(self.buffer,(1,2,0)),axis=2)\n",
    "    \n",
    "class GymEnvironment():\n",
    "    def __init__(self,name,conf):\n",
    "        self.env = gym.make(name)            # Initialize Gym environment\n",
    "        self.buffer = GameScreen(conf)\n",
    "        self.screen_width = conf['screen_width']         \n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.random_start = conf['random_start']\n",
    "        self.action_repeat = conf['action_repeat']\n",
    "        self.pool_frame_size = conf['pool_frame_size']\n",
    "        self.display = conf['display']\n",
    "        \n",
    "    def new_game(self):\n",
    "        self._observation = self.env.reset()\n",
    "        \n",
    "        for i in range(self.pool_frame_size):\n",
    "            self.buffer.add(self.observation)\n",
    "            \n",
    "        self.render()        \n",
    "        return self.screen\n",
    "    \n",
    "    def new_random_game(self):  # Starts a random new game doing \n",
    "        _ = self.new_game()\n",
    "        terminal = False\n",
    "        for _ in xrange(random.randint(0,self.random_start-1)):\n",
    "            self._observation, reward, terminal, _ = self.env.step(0)\n",
    "        \n",
    "        self.render()\n",
    "        return self.screen, 0, terminal\n",
    "        \n",
    "    def execute_action(self,action,is_training=True):\n",
    "        # This function execute the selected action for 'action_repeat' number of times and returns the cumulative reward\n",
    "        # and final state\n",
    "        cum_reward = 0\n",
    "        start_lives = self.num_lives\n",
    "        \n",
    "        for _ in xrange(self.action_repeat):\n",
    "            self._observation, reward, terminal, _ = self.env.step(action)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            if is_training and start_lives > self.num_lives:\n",
    "                terminal = True\n",
    "                \n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        self.render()\n",
    "        \n",
    "        return self.screen, cum_reward, terminal\n",
    "    \n",
    "    @property\n",
    "    def screen(self):\n",
    "        return self.buffer.get()\n",
    "        \n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.env.action_space.n        # Number of available actions\n",
    "\n",
    "    @property\n",
    "    def num_lives(self):\n",
    "        return self.env.ale.lives()\n",
    "    \n",
    "    @property\n",
    "    def observation(self):     # Method to resize the screen provided by gym to the desired values\n",
    "        return cv2.resize(cv2.cvtColor(self._observation,cv2.COLOR_RGB2GRAY)/255.,(self.screen_width,self.screen_height))\n",
    "    \n",
    "    def render(self):    # Renders the environment only if display == True\n",
    "        if self.display:\n",
    "            self.env.render()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "This class will allow us to store the experiences and to take random samples to update our target network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experience_Replay():\n",
    "    def __init__(self,conf):\n",
    "        self.memory_size = conf['memory_size']\n",
    "        # These are the arrays where we will store the experiences\n",
    "        self._experience = {}\n",
    "        self.batch_size = conf['batch_size']       \n",
    "        self.current = 0     # Pointer to the current saving location\n",
    "        self.count = 0       # Number of collected experiences\n",
    "        \n",
    "    def add(self, experience):\n",
    "        '''\n",
    "        Stores experience: experience is a tuple of (s1,a,r,s2,t)\n",
    "        '''        \n",
    "        if self.current in self._experience:\n",
    "            del self._experience[self.current]\n",
    "        self._experience[self.current] = experience\n",
    "        \n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "                    \n",
    "    def retrieve(self,indices):\n",
    "        '''\n",
    "        Get experiences from indices\n",
    "        '''\n",
    "        return [self._experience[v] for v in indices]\n",
    "        \n",
    "    def sample_from_replay(self):\n",
    "        # Randomly sampling from experiences (Standard implementation)\n",
    "        indexes = []\n",
    "        while len(indexes) < self.batch_size:\n",
    "            while True:\n",
    "                index = random.randint(1,self.count-1)\n",
    "                '''\n",
    "                # If index wraps over terminal state, get new one\n",
    "                if self._experience[index][4]:\n",
    "                    continue\n",
    "                '''\n",
    "                # Use the index otherwise\n",
    "                break\n",
    "            indexes.append(index)\n",
    "            \n",
    "        experience_batch = self.retrieve(indexes)\n",
    "        state, action , reward, next_state, terminal = map(np.array, zip(*experience_batch))\n",
    "        \n",
    "        return state, action, reward, next_state, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "This class will allow us to stack the last K screens to use them as the input to the network (history of states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, conf):\n",
    "        self.history = np.zeros([conf['history_length'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = screen\n",
    "        \n",
    "    def get(self):\n",
    "        return np.transpose(self.history,(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvNet_Estimator():\n",
    "    '''Q-value estimator neural network\n",
    "       This architecture will be used both for the Q-network and the Target network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,conf,num_actions,scope='estimator',summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        self.num_actions = num_actions\n",
    "        self.screen_width = conf['screen_width']\n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.history_length = conf['history_length']\n",
    "        self.ep_min = conf['ep_min']\n",
    "        self.ep_start = conf['ep_start']\n",
    "        self.ep_end_time = conf['ep_end_time']\n",
    "        self.learn_start = conf['learn_start']\n",
    "        self.batch_size = conf['batch_size']\n",
    "        self.learning_rate = conf['learning_rate']\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "                \n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        Building the network architecture\n",
    "        '''\n",
    "        initializer = tf.constant_initializer(0.0)  # Change to None to remove biases from conv layers \n",
    "        \n",
    "        # Our input: 4(or history_length) Frames taken from the environment\n",
    "        self.X_pl = tf.placeholder(tf.float32,[None, self.screen_height, self.screen_width, self.history_length],name='X')\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(tf.float32,[None],name='y')\n",
    "        # Integer id of selected action\n",
    "        self.action_pl = tf.placeholder(tf.int32,[None],name='action')\n",
    "        \n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(self.X_pl,32,8,4,padding = 'VALID',\n",
    "                                         activation_fn=tf.nn.relu,biases_initializer=initializer)\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1,64,4,2,padding = 'VALID',\n",
    "                                         activation_fn=tf.nn.relu,biases_initializer=initializer)\n",
    "        conv3 = tf.contrib.layers.conv2d(conv2,63,3,1,padding = 'VALID',\n",
    "                                         activation_fn=tf.nn.relu,biases_initializer=initializer)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened,512,activation_fn=tf.nn.relu,biases_initializer=initializer)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1,self.num_actions,biases_initializer=initializer)\n",
    "        self.best_action = tf.argmax(self.predictions,dimension=1)\n",
    "        \n",
    "        # One hot of the action which was taken\n",
    "        action_one_hot = tf.one_hot(self.action_pl,self.num_actions,1.0,0.0)\n",
    "        # Get the prediction of the chosen actions only\n",
    "        self.action_predictions = tf.reduce_sum(self.predictions * action_one_hot,reduction_indices=1)\n",
    "        \n",
    "        # *** NEW: DOUBLE Q-LEARNING!! ***\n",
    "        self.outputs_idx = tf.placeholder(tf.int32,[None,None])\n",
    "        self.outputs_with_idx = tf.gather_nd(self.predictions,self.outputs_idx)\n",
    "        # *** END OF NEW SEGMENT\n",
    "        \n",
    "        # Calculate the loss\n",
    "        self.delta = self.y_pl-self.action_predictions\n",
    "        self.clipped_delta = tf.clip_by_value(self.delta,-1.0,1.0)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.clipped_delta))\n",
    "        \n",
    "        # Optimizer using parameteres from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate,momentum=0.95,epsilon=0.01)\n",
    "        self.train_op = self.optimizer.minimize(self.loss,global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.merge_summary([\n",
    "                tf.scalar_summary('loss', self.loss),\n",
    "                tf.histogram_summary('q_values_hist',self.predictions),\n",
    "                tf.scalar_summary('max_q_value',tf.reduce_max(self.predictions))\n",
    "            ])      \n",
    "      \n",
    "    # *** NEW: DOUBLE Q-LEARNING!! ***\n",
    "    def get_best_action(self,sess,state):\n",
    "        return sess.run(self.best_action,{self.X_pl:state})\n",
    "    \n",
    "    def get_output_with_idx(self,sess,state,idx):\n",
    "        return sess.run(self.outputs_with_idx,{self.X_pl:state, self.outputs_idx:idx})    \n",
    "    # *** END OF NEW SEGMENT\n",
    "                \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predicts action values.\n",
    "        '''\n",
    "        return sess.run(self.predictions,{self.X_pl:state})\n",
    "    \n",
    "    def determine_action(self,sess,state,current_step,test_ep=None):\n",
    "        '''\n",
    "        Predicts action based on q values for current state and exploration strategy\n",
    "        '''\n",
    "        # Calculate exploration prob. epsilon => This is a decaying epsilon starting at 1 and decreasing until 0.1 once the learning process start\n",
    "        ep = test_ep or (self.ep_min + max(0.,(self.ep_start-self.ep_min)*(self.ep_end_time-max(0.,current_step-self.learn_start))/self.ep_end_time))\n",
    "        if random.random() < ep:\n",
    "            # Explore: random action\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:            \n",
    "            action = sess.run(self.best_action,{self.X_pl:[state]})[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self,sess,state,action,target):\n",
    "        '''\n",
    "        Updates the estimator towards the given targets\n",
    "        '''\n",
    "        feed_dict = {self.X_pl:state, self.y_pl:target, self.action_pl:action}\n",
    "        q_t, summaries, _, loss = sess.run([self.predictions, self.summaries,self.train_op,self.loss],feed_dict)\n",
    "\n",
    "        return q_t, loss, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,conf,environment,sess):\n",
    "        self.conf = conf\n",
    "        self.sess = sess\n",
    "        self.env = environment\n",
    "        self.exp_replay = Experience_Replay(self.conf)\n",
    "        self.history = History(self.conf)\n",
    "        self.checkpoint_dir = conf['checkpoint_dir']\n",
    "        self.log_performance_file = conf['log_performance_file']\n",
    "        self.log_dir = conf['log_dir']\n",
    "        self.history_length = conf['history_length']\n",
    "        self.learn_start = conf['learn_start']  \n",
    "        self.train_frequency = conf['train_frequency']\n",
    "        self.target_q_update_step = conf['target_q_update_step']\n",
    "        self.eval_freq = conf['eval_freq']\n",
    "        self.eval_steps = conf['eval_steps']\n",
    "        self.ep_min = conf['ep_min']\n",
    "        self.max_steps = conf['max_steps']\n",
    "        self.min_reward = conf['min_reward']\n",
    "        self.max_reward = conf['max_reward']\n",
    "        self.gamma = conf['gamma']\n",
    "        \n",
    "        self.q_estimator = ConvNet_Estimator(conf_parameters,self.env.action_size,scope='q',summaries_dir=self.log_dir)\n",
    "        self.target_estimator = ConvNet_Estimator(conf_parameters,self.env.action_size,scope='target_q')\n",
    "        \n",
    "        with tf.variable_scope('step'):\n",
    "            self.step_op = tf.Variable(0,trainable=False,name='step')\n",
    "            self.step_input = tf.placeholder(tf.int32,None,name='step_input')\n",
    "            self.step_assign_op = self.step_op.assign(self.step_input)\n",
    "        \n",
    "        # Add summaries\n",
    "        with tf.variable_scope('summary'):\n",
    "            scalar_summary_tags = ['average.reward', 'average.loss', 'average.q', 'episode.max reward', 'episode.min reward',\\\n",
    "                                  'episode.avg reward', 'episode.num of games']\n",
    "            self.summary_placeholders = {}\n",
    "            self.summary_ops = {}\n",
    "            for tag in scalar_summary_tags:\n",
    "                self.summary_placeholders[tag] = tf.placeholder(tf.float32,None,name=tag.replace(' ','_'))\n",
    "                self.summary_ops[tag] = tf.scalar_summary(tag, self.summary_placeholders[tag])\n",
    "            \n",
    "            histogram_summary_tags = ['episode.rewards']\n",
    "            for tag in histogram_summary_tags:\n",
    "                self.summary_placeholders[tag] = tf.placeholder(tf.float32,None,name=tag.replace(' ','_'))\n",
    "                self.summary_ops[tag] = tf.histogram_summary(tag, self.summary_placeholders[tag])\n",
    "                \n",
    "            self.writer = tf.train.SummaryWriter(self.log_dir,self.sess.graph)\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        self.saver = tf.train.Saver(max_to_keep = 3)\n",
    "        \n",
    "        self.load_model()\n",
    "        self.update_target_q_network(self.sess)\n",
    "\n",
    "    def update_target_q_network(self,sess):\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(self.q_estimator.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(self.target_estimator.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params,e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            update_ops.append(op)\n",
    "\n",
    "        sess.run(update_ops)\n",
    "            \n",
    "    def load_model(self,step=None):\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print('[*] Loading Checkpoint {}...\\n'.format(latest_checkpoint))\n",
    "            self.saver.restore(self.sess,latest_checkpoint)\n",
    "        else:\n",
    "            print('[!] Could not find any previous checkpoint')\n",
    "        \n",
    "    def save_model(self,step=None):\n",
    "        f = open(self.log_performance_file, 'a')\n",
    "        f.write('[*] Saving checkpoints...')\n",
    "        f.close()\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(self.checkpoint_dir,'model'),global_step=step)    \n",
    "        \n",
    "    def train(self):\n",
    "        start_step = self.step_op.eval()\n",
    "        max_avg_ep_reward = 0\n",
    "        screen,_,terminal = self.env.new_random_game()\n",
    "        \n",
    "        # Stacking the screen in the first input buffer\n",
    "        for _ in range(self.history_length):\n",
    "            self.history.add(screen)\n",
    "            \n",
    "        # Training:\n",
    "        for self.step in tqdm(range(start_step,self.max_steps),ncols=70,initial=start_step):               \n",
    "            # 1. Predict: Use our training network to select an action\n",
    "            action = self.q_estimator.determine_action(self.sess,self.history.get(),self.step)\n",
    "            \n",
    "            # 2. Execute the action\n",
    "            screen, reward, terminal = self.env.execute_action(action,is_training=True)\n",
    "            \n",
    "            # 3. New observation\n",
    "            self.observe(screen,reward,action,terminal)\n",
    "            \n",
    "            if terminal:\n",
    "                screen,_,terminal = self.env.new_random_game()\n",
    "                for _ in range(self.history_length):\n",
    "                    self.history.add(screen)\n",
    "                \n",
    "            if self.step >= self.learn_start and self.step % self.eval_freq == 0:\n",
    "                # New game\n",
    "                screen = self.env.new_game()\n",
    "                for _ in range(self.history_length):\n",
    "                    self.history.add(screen)\n",
    "                # \n",
    "                total_reward, ep_reward = 0., 0.\n",
    "                n_rewards = 0\n",
    "                n_episodes = 0\n",
    "                ep_rewards = []\n",
    "                \n",
    "                for eval_step in range(self.eval_steps):\n",
    "                    # Take action with exploration e= 0.05\n",
    "                    action = self.q_estimator.determine_action(self.sess,self.history.get(),self.step,test_ep=0.05)\n",
    "                    # Play game in test mode (Episodes do not end when losing a life)\n",
    "                    screen, reward, terminal = self.env.execute_action(action,is_training=False)\n",
    "                    # Clip reward\n",
    "                    reward = max(self.min_reward,min(self.max_reward,reward))\n",
    "                    # Record every reward\n",
    "                    ep_reward += reward\n",
    "                    if reward != 0:\n",
    "                        n_rewards += 1\n",
    "                    \n",
    "                    if terminal:\n",
    "                        ep_rewards.append(ep_reward)\n",
    "                        total_reward += ep_reward\n",
    "                        ep_reward = 0\n",
    "                        n_episodes += 1\n",
    "                        screen,_,terminal = self.env.new_random_game()\n",
    "                                                                \n",
    "                q_t,avg_loss,summaries = self.q_learning_mini_batch()\n",
    "                \n",
    "                avg_reward = total_reward / max(n_rewards,1)\n",
    "                avg_q = q_t.mean()\n",
    "\n",
    "                try:\n",
    "                    max_ep_reward = np.max(ep_rewards)\n",
    "                    min_ep_reward = np.min(ep_rewards)\n",
    "                    avg_ep_reward = np.mean(ep_rewards)\n",
    "                except:\n",
    "                    max_ep_reward, min_ep_reward, avg_ep_reward = 0,0,0\n",
    "                \n",
    "                f = open(self.log_performance_file, 'a')\n",
    "                f.write('\\nAvg. reward: %.4f, Avg. loss: %.6f, Avg. Q: %3.6f' % (avg_reward, avg_loss, avg_q))\n",
    "                f.write('Avg. Ep. Reward: %.4f, Max Ep. Reward: %.4f, Min Ep. Reward: %.4f, # Game: %d' \\\n",
    "                        % (avg_ep_reward, max_ep_reward, min_ep_reward, n_episodes))\n",
    "                f.close()\n",
    "\n",
    "                if max_avg_ep_reward * 1.1 <= avg_ep_reward:\n",
    "                    # There is at least a 10% improvement\n",
    "                    self.step_assign_op.eval({self.step_input: self.step+1})\n",
    "                    self.save_model(self.step+1)\n",
    "                    max_avg_ep_reward = avg_ep_reward\n",
    "\n",
    "                self.inject_summary({\n",
    "                        'average.reward' : avg_reward,\n",
    "                        'average.loss': avg_loss,\n",
    "                        'average.q': avg_q,\n",
    "                        'episode.max reward':max_ep_reward,\n",
    "                        'episode.min reward':min_ep_reward,\n",
    "                        'episode.avg reward':avg_ep_reward,\n",
    "                        'episode.num of games':n_episodes,\n",
    "                        'episode.rewards': ep_rewards,\n",
    "                    })\n",
    "    \n",
    "    def observe(self,screen,reward,action,terminal):\n",
    "        # Clip reward\n",
    "        reward = max(self.min_reward,min(self.max_reward,reward))\n",
    "        \n",
    "        prev_state = self.history.get()\n",
    "        # Add to history\n",
    "        self.history.add(screen)\n",
    "        # Add to exp. replay\n",
    "        self.exp_replay.add((prev_state,reward,action,self.history.get(),terminal))\n",
    "        \n",
    "        if self.step > self.learn_start:\n",
    "            # If it is time to train the network\n",
    "            if self.step % self.train_frequency == 0:\n",
    "                _,_,summaries = self.q_learning_mini_batch()\n",
    "                self.writer.add_summary(summaries,self.step)\n",
    "                \n",
    "            # If it is time to update Target network\n",
    "            if self.step % self.target_q_update_step == self.target_q_update_step -1:\n",
    "                self.update_target_q_network(self.sess)\n",
    "        \n",
    "    def q_learning_mini_batch(self):\n",
    "        init_state, action, reward, end_state, terminal = self.exp_replay.sample_from_replay()\n",
    "\n",
    "        # *** NEW: DOUBLE Q-LEARNING!! ***\n",
    "        best_action_t_plus_1 = self.q_estimator.get_best_action(self.sess,end_state)\n",
    "        q_t_plus_1_with_pred_action = self.target_estimator.get_output_with_idx(\n",
    "            self.sess,end_state,[[idx,pred_a] for idx,pred_a in enumerate(best_action_t_plus_1)])\n",
    "        terminal = np.array(terminal) + 0.\n",
    "        target_q_t = (1. - terminal) * self.gamma * q_t_plus_1_with_pred_action + reward\n",
    "        # *** END OF NEW SEGMENT\n",
    "\n",
    "        q_t, loss, summaries = self.q_estimator.update(self.sess,init_state,action,target_q_t)\n",
    "\n",
    "        return q_t, loss, summaries       \n",
    "            \n",
    "    def inject_summary(self,tag_dir):\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dir.keys()], {\n",
    "                self.summary_placeholders[tag]: value for tag , value in tag_dir.items()\n",
    "            })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str,self.step)\n",
    "            \n",
    "    def play(self,n_step=10000,n_episode=100,test_epsilon=None):\n",
    "        if test_epsilon == None:\n",
    "            test_epsilon = ep_min\n",
    "        \n",
    "        test_history = History()\n",
    "        best_reward, best_idx = 0, 0\n",
    "        for idx in xrange(n_episode):\n",
    "            screen,_,_,_ = self.env.new_random_game()\n",
    "            current_reward = 0\n",
    "            \n",
    "            for _ in range(history_length):\n",
    "                test_history.add(screen)\n",
    "                \n",
    "            for t in tqdm(range(n_step),ncols=70):\n",
    "                # 1. Predict\n",
    "                action = self.q_estimator.determine_action(self.sess,test_history.get(),0,test_epsilon)\n",
    "            \n",
    "                # 2. Execute the action\n",
    "                screen, reward, terminal = self.env.execute_action(action,is_training=False)                \n",
    "                \n",
    "                # 3. New observation\n",
    "                test_history.add(screen)\n",
    "                \n",
    "                current_reward += reward\n",
    "                if terminal:\n",
    "                    break\n",
    "            \n",
    "            if current_reward > best_reward:\n",
    "                best_reward = current_reward\n",
    "                best_idx = idx\n",
    "                \n",
    "            print('='*30)\n",
    "            print('[%d] Best reward: %d' % (best_idx, best_reward))\n",
    "            print('='*30)\n",
    "            \n",
    "        self.env.env.render(close=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Main code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:00,029] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:71 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:00,830] From <ipython-input-6-4ec4eafd975f>:71 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:72 in _build_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:00,850] From <ipython-input-6-4ec4eafd975f>:72 in _build_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:00,881] From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:00,909] From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:71 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,308] From <ipython-input-6-4ec4eafd975f>:71 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:72 in _build_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,327] From <ipython-input-6-4ec4eafd975f>:72 in _build_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,354] From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,394] From <ipython-input-6-4ec4eafd975f>:73 in _build_model.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,423] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,442] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,464] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,560] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,578] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,593] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,615] From <ipython-input-7-24aa02969167>:38 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:43 in __init__.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,637] From <ipython-input-7-24aa02969167>:43 in __init__.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:45 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:01,657] From <ipython-input-7-24aa02969167>:45 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-24aa02969167>:47 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-21 15:14:02,811] From <ipython-input-7-24aa02969167>:47 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Could not find any previous checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        0%|                     | 101/50000000 [00:00<13:48:47, 1005.47it/s]  0%|                    | 59997/50000000 [04:40<64:47:24, 214.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]\n",
      " [ 0.          0.06459868  0.02126648  0.02856881  0.05593806  0.07656018]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                 | 80009/50000000 [15:22<378337:52:14, 27.28s/it]  0%|                  | 60009/50000000 [06:32<96047:25:57,  6.92s/it]  0%|                    | 80000/50000000 [13:31<140:37:51, 98.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]\n",
      " [ 0.          0.07648701  0.01996765  0.02852895  0.05529194  0.07785739]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                    | 82309/50000000 [16:09<282:41:08, 49.05it/s]  0%|                 | 80025/50000000 [15:22<185525:31:22, 13.38s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01b8cd45b647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-24aa02969167>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# 3. New observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-24aa02969167>\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, screen, reward, action, terminal)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# If it is time to train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_learning_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-24aa02969167>\u001b[0m in \u001b[0;36mq_learning_mini_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# *** END OF NEW SEGMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mq_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_q_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4ec4eafd975f>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, state, action, target)\u001b[0m\n\u001b[1;32m    107\u001b[0m         '''\n\u001b[1;32m    108\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_pl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mq_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/camilog/anaconda2/envs/Neptuno/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/camilog/anaconda2/envs/Neptuno/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/camilog/anaconda2/envs/Neptuno/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/camilog/anaconda2/envs/Neptuno/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/camilog/anaconda2/envs/Neptuno/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_train = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    env = GymEnvironment('Breakout-v0',conf_parameters)\n",
    "    agent = Agent(conf_parameters,env,sess)\n",
    "    \n",
    "    if is_train:\n",
    "        display = False\n",
    "        agent.train()\n",
    "    else:\n",
    "        display = True\n",
    "        agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-12 16:46:48,065] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Could not find any previous checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                    | 14333/50000000 [01:25<87:28:39, 158.73it/s]"
     ]
    }
   ],
   "source": [
    "is_train = True\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "gpu_config.log_device_placement = True\n",
    "\n",
    "with tf.Session(config=gpu_config) as sess:\n",
    "    env = GymEnvironment('Breakout-v0')\n",
    "    agent = Agent(env,sess)\n",
    "    \n",
    "    if is_train:\n",
    "        display = False\n",
    "        agent.train()\n",
    "    else:\n",
    "        display = False\n",
    "        agent.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Neptuno]",
   "language": "python",
   "name": "conda-env-Neptuno-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
