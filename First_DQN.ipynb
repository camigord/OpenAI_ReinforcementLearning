{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "screen_width = 84\n",
    "screen_height = 84\n",
    "history_length = 4\n",
    "\n",
    "memory_size = 1000000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "learning_rate = 0.0005\n",
    "learning_rate_decay = 0.96\n",
    "learning_rate_decay_step = 50000\n",
    "\n",
    "max_steps = 50000000\n",
    "learn_start = 50000\n",
    "\n",
    "# Exploration parameters\n",
    "ep_min = 0.1\n",
    "ep_start = 1.0\n",
    "ep_end_time = memory_size\n",
    "\n",
    "# Update Target network\n",
    "target_q_update_step = 10000\n",
    "\n",
    "# Train batch\n",
    "train_frequency = 4\n",
    "# Print performance\n",
    "test_step = 50000\n",
    "\n",
    "# Clip rewards\n",
    "min_reward = -1.0\n",
    "max_reward = 1.0\n",
    "\n",
    "# How many times should the same action be taken (avoids taking a new decision after every environment iteration)\n",
    "action_repeat = 4\n",
    "\n",
    "# Whether or not to render the environment \n",
    "display = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GymEnvironment():\n",
    "    def __init__(self,name):\n",
    "        self.env = gym.make(name)\n",
    "        self.action_repeat = action_repeat\n",
    "        \n",
    "    def execute_action(self,action,is_training=True):\n",
    "        cum_reward = 0\n",
    "        start_lives = self.env.ale.lives()\n",
    "        \n",
    "        for _ in xrange(self.action_repeat):\n",
    "            screen, reward, terminal, _ = self.env.step(action)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            if is_training and start_lives > self.env.ale.lives():\n",
    "                cum_reward -= 1\n",
    "                terminal = True\n",
    "                \n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        reward = cum_reward\n",
    "        \n",
    "        if display:\n",
    "            self.env.render()\n",
    "        \n",
    "        screen = cv2.resize(cv2.cvtColor(screen,cv2.COLOR_RGB2GRAY)/255.,(screen_width,screen_height))\n",
    "        return screen, reward, terminal\n",
    "    \n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def new_game(self):\n",
    "        screen = self.env.reset()\n",
    "        screen = cv2.resize(cv2.cvtColor(screen,cv2.COLOR_RGB2GRAY)/255.,(screen_width,screen_height))\n",
    "        \n",
    "        return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'screen = env.reset()\\nprint(screen.shape)\\nenv.render()\\nenv.render(close=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''screen = env.reset()\n",
    "print(screen.shape)\n",
    "env.render()\n",
    "env.render(close=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "This class will allow us to store the experiences and to take random samples to update our target network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experience_Buffer():\n",
    "    def __init__(self,memory_size = 50000):\n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "        self.actions = np.empty(self.memory_size,dtype=np.uint8)\n",
    "        self.rewards = np.empty(self.memory_size,dtype=np.integer)\n",
    "        self.screens = np.empty((self.memory_size, screen_height, screen_width),dtype=np.float16)\n",
    "        self.terminals = np.empty(self.memory_size,dtype=np.bool)\n",
    "        self.prestates = np.empty((batch_size,history_length,screen_height, screen_width),dtype=np.float16)\n",
    "        self.poststates = np.empty((batch_size,history_length,screen_height, screen_width),dtype=np.float16)\n",
    "        self.current = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def add(self, screen, reward, action, terminal):\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.screens[self.current,...] = screen\n",
    "        self.terminals[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        \n",
    "    def getState(self,index):\n",
    "        index = index % self.count\n",
    "        # If index is not in the beginning, just use simple slicing\n",
    "        if index >= history_length-1:\n",
    "            return self.screens[(index-(history_length-1)):(index+1),...]\n",
    "        # Otherwise determine the list of indexes which need to be returned\n",
    "        else:\n",
    "            indexes = [(index-i) % self.count for i in reversed(range(history_length))]\n",
    "            return self.screens[indexes,...]\n",
    "        \n",
    "    def sample_from_replay(self):\n",
    "        # Sample random indexes\n",
    "        indexes = []\n",
    "        while len(indexes) < batch_size:\n",
    "            while True:\n",
    "                index = random.randint(history_length,self.count-1)\n",
    "                # If index wraps over current pointer, get new one\n",
    "                if index >= self.current and index - history_length < self.current:\n",
    "                    continue\n",
    "                # If index wraps over terminal state, get new one\n",
    "                if self.terminals[(index-history_length):index].any():\n",
    "                    continue\n",
    "                # Use the index otherwise\n",
    "                break\n",
    "            self.prestates[len(indexes),...] = self.getState(index-1)\n",
    "            self.poststates[len(indexes),...] = self.getState(index)\n",
    "            indexes.append(index)\n",
    "            \n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        terminals = self.terminals[indexes]\n",
    "        \n",
    "        return np.transpose(self.prestates,(0,2,3,1)),actions,rewards,np.transpose(self.poststates,(0,2,3,1)),terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "This class will allow us to stack the last K screens to use them as the input to the network (history of states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self):\n",
    "        self.history = np.zeros([history_length,screen_height,screen_width],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = screen\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history *= 0\n",
    "        \n",
    "    def get(self):\n",
    "        return np.transpose(self.history,(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'Models/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "log_dir = 'Models/Logs/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "def conv2d(x,output_dim,kernel_size,stride,\n",
    "           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "           activation_fn = tf.nn.relu,\n",
    "           padding = 'VALID',\n",
    "           name = 'conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride = [1,stride[0],stride[1],1]\n",
    "        kernel_shape = [kernel_size[0],kernel_size[1],x.get_shape()[-1],output_dim]\n",
    "        \n",
    "        w = tf.get_variable('w',kernel_shape,tf.float32,initializer=initializer)\n",
    "        conv = tf.nn.conv2d(x,w,stride,padding)\n",
    "        b = tf.get_variable('biases',[output_dim],initializer=tf.constant_initializer(0.0))\n",
    "        out = activation_fn(tf.nn.bias_add(conv,b))\n",
    "        \n",
    "    return out,w,b       \n",
    "\n",
    "def linear(input_,output_size,stddev=0.02,bias_start=0.0,activation_fn=None,name='linear'):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('Matrix',[shape[1],output_size],tf.float32,\n",
    "                            tf.random_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable('bias',[output_size],initializer=tf.constant_initializer(bias_start))\n",
    "        \n",
    "        out = tf.nn.bias_add(tf.matmul(input_,w),b)\n",
    "        \n",
    "    if activation_fn!=None:\n",
    "        return activation_fn(out), w, b\n",
    "    else:\n",
    "        return out,w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,environment,sess):\n",
    "        self.sess = sess\n",
    "        self.env = environment\n",
    "        self.exp_replay = Experience_Buffer(memory_size)\n",
    "        self.history = History()\n",
    "        \n",
    "        with tf.variable_scope('step'):\n",
    "            self.step_op = tf.Variable(0,trainable=False,name='step')\n",
    "            self.step_input = tf.placeholder(tf.int32,None,name='step_input')\n",
    "            self.step_assign_op = self.step_op.assign(self.step_input)\n",
    "        \n",
    "        # Build Deep network\n",
    "        self.build_dqn()\n",
    "        \n",
    "    def build_dqn(self):\n",
    "        self.w = {}\n",
    "        self.t_w = {}\n",
    "        \n",
    "        initializer = tf.truncated_normal_initializer(0, 0.02)\n",
    "        activation_fn = tf.nn.relu\n",
    "        \n",
    "        # TRAINING NETWORK\n",
    "        with tf.variable_scope('prediction'):\n",
    "            self.input_state = tf.placeholder(tf.float32,[None, screen_height, screen_width, history_length],\n",
    "                                              name='input_state')\n",
    "            self.l1, self.w['l1_w'], self.w['l1_b'] = conv2d(self.input_state,32,[8,8],[4,4],initializer, \n",
    "                                                             activation_fn, name='l1')\n",
    "            self.l2, self.w['l2_w'], self.w['l2_b'] = conv2d(self.l1,64,[4,4],[2,2],initializer, \n",
    "                                                             activation_fn, name='l2')\n",
    "            self.l3, self.w['l3_w'], self.w['l3_b'] = conv2d(self.l2,64,[3,3],[1,1],initializer, \n",
    "                                                             activation_fn, name='l3')\n",
    "            \n",
    "            shape = self.l3.get_shape().as_list()\n",
    "            self.l3_flat = tf.reshape(self.l3,[-1,reduce(lambda x,y: x*y,shape[1:])])\n",
    "            \n",
    "            # Standard DQN implementation\n",
    "            self.l4, self.w['l4_w'], self.w['l4_b'] = linear(self.l3_flat,512,activation_fn=activation_fn,name='l4')\n",
    "            self.q, self.w['q_w'],self.w['q_b'] = linear(self.l4,self.env.action_size,name='q')\n",
    "            \n",
    "            self.q_action = tf.argmax(self.q,dimension=1)\n",
    "            \n",
    "            # Add output summaries\n",
    "            q_summary = []\n",
    "            avg_q = tf.reduce_mean(self.q,0)  # Mean q_value per action for each batch\n",
    "            for idx in xrange(self.env.action_size):\n",
    "                q_summary.append(tf.histogram_summary('q/%s' % idx, avg_q[idx]))\n",
    "            self.q_summary = tf.merge_summary(q_summary,'q_summary')\n",
    "                           \n",
    "        # TARGET NETWORK\n",
    "        with tf.variable_scope('target'):\n",
    "            self.target_state = tf.placeholder(tf.float32,[None, screen_height, screen_width, history_length],\n",
    "                                              name='target_state')\n",
    "            self.target_l1, self.t_w['l1_w'], self.t_w['l1_b'] = conv2d(self.target_state,32,[8,8],[4,4],initializer, \n",
    "                                                             activation_fn, name='target_l1')\n",
    "            self.target_l2, self.t_w['l2_w'], self.t_w['l2_b'] = conv2d(self.target_l1,64,[4,4],[2,2],initializer, \n",
    "                                                             activation_fn, name='target_l2')\n",
    "            self.target_l3, self.t_w['l3_w'], self.t_w['l3_b'] = conv2d(self.target_l2,64,[3,3],[1,1],initializer, \n",
    "                                                             activation_fn, name='target_l3')\n",
    "            \n",
    "            shape = self.target_l3.get_shape().as_list()\n",
    "            self.target_l3_flat = tf.reshape(self.target_l3,[-1,reduce(lambda x,y: x*y,shape[1:])])\n",
    "            \n",
    "            # Standard DQN\n",
    "            self.target_l4, self.t_w['l4_w'], self.t_w['l4_b'] = linear(self.target_l3_flat,512,\n",
    "                                                                    activation_fn=activation_fn,name='target_l4')\n",
    "            self.target_q, self.t_w['q_w'],self.t_w['q_b'] = linear(self.target_l4,self.env.action_size,\n",
    "                                                                    name='target_q')\n",
    "            \n",
    "            self.target_q_idx = tf.placeholder(tf.int32,[None,None],'outputs_idx')\n",
    "            self.target_q_with_idx = tf.gather_nd(self.target_q,self.target_q_idx)  # Gets q value according to index\n",
    "        \n",
    "        # COPY TRAINING NETWORK INTO TARGET\n",
    "        with tf.variable_scope('pred_to_target'):\n",
    "            self.t_w_input = {}\n",
    "            self.t_w_assign_op = {}\n",
    "            \n",
    "            for name in self.w.keys():\n",
    "                self.t_w_input[name] = tf.placeholder(tf.float32,self.t_w[name].get_shape().as_list(),name=name)\n",
    "                self.t_w_assign_op[name] = self.t_w[name].assign(self.t_w_input[name])\n",
    "                    \n",
    "        # OPTIMIZER\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.target_q_t = tf.placeholder(tf.float32,[None],name='target_q_t')\n",
    "            self.action = tf.placeholder(tf.int64,[None],name='action')\n",
    "            \n",
    "            # One hot of the action which was taken\n",
    "            action_one_hot = tf.one_hot(self.action,self.env.action_size,1.0,0.0,name='action_one_hot')\n",
    "            # Extract the q_value of the action\n",
    "            q_acted = tf.reduce_sum(self.q * action_one_hot,reduction_indices=1,name='q_acted')\n",
    "            \n",
    "            self.delta = self.target_q_t - q_acted      # Error\n",
    "            self.clipped_delta = tf.clip_by_value(self.delta,-1.0,1.0,name='clipped_delta')\n",
    "            \n",
    "            # PAY ATTENTION TO THIS ONE!\n",
    "            self.global_step = tf.Variable(0,trainable=False)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.clipped_delta),name='loss')\n",
    "            self.learning_rate_step = tf.placeholder(tf.int64,None,name='learning_rate_step')\n",
    "            self.learning_rate_op = tf.maximum(0.00025,tf.train.exponential_decay(\n",
    "                learning_rate,self.learning_rate_step,learning_rate_decay_step,learning_rate_decay,staircase=True))\n",
    "            self.optim = tf.train.RMSPropOptimizer(self.learning_rate_op,momentum=0.95,epsilon=0.01).minimize(self.loss)\n",
    "            \n",
    "        # Add summaries\n",
    "        with tf.variable_scope('summary'):\n",
    "            scalar_summary_tags = ['average.reward', 'average.loss', 'average.q', 'episode.max reward', 'episode.min reward',\\\n",
    "                                  'episode.avg reward', 'episode.num of games', 'training.learning_rate']\n",
    "            self.summary_placeholders = {}\n",
    "            self.summary_ops = {}\n",
    "            for tag in scalar_summary_tags:\n",
    "                self.summary_placeholders[tag] = tf.placeholder(tf.float32,None,name=tag.replace(' ','_'))\n",
    "                self.summary_ops[tag] = tf.scalar_summary(tag, self.summary_placeholders[tag])\n",
    "            \n",
    "            histogram_summary_tags = ['episode.rewards','episode.actions']\n",
    "            for tag in histogram_summary_tags:\n",
    "                self.summary_placeholders[tag] = tf.placeholder(tf.float32,None,name=tag.replace(' ','_'))\n",
    "                self.summary_ops[tag] = tf.histogram_summary(tag, self.summary_placeholders[tag])\n",
    "                \n",
    "            self.writer = tf.train.SummaryWriter(log_dir,self.sess.graph)\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        self._saver = tf.train.Saver(self.w.values() + [self.step_op], max_to_keep = 10)\n",
    "        \n",
    "        self.load_model()\n",
    "        self.update_target_q_network()\n",
    "                    \n",
    "    def update_target_q_network(self):\n",
    "        for name in self.w.keys():\n",
    "            self.t_w_assign_op[name].eval({self.t_w_input[name]:self.w[name].eval()})\n",
    "            \n",
    "    def load_model(self,step=None):\n",
    "        print('[*] Loading Checkpoints...')\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            fname = os.path.join(checkpoint_dir,ckpt_name)\n",
    "            self._saver.restore(self.sess,fname)\n",
    "            print('[*] Load succesfull: %s' % fname)\n",
    "            return True\n",
    "        else:\n",
    "            print('[!] Load failed: %s' % checkpoint_dir)\n",
    "            return False\n",
    "        \n",
    "    def save_model(self,step=None):\n",
    "        print('[*] Saving checkpoints...')\n",
    "        self._saver.save(self.sess,checkpoint_dir,global_step=step)    \n",
    "        \n",
    "    def train(self):\n",
    "        start_step = self.step_op.eval()\n",
    "        print('ok')\n",
    "        # VERIFY THE NEED OF THE FOLLOWING VARIABLES\n",
    "        num_game, self.update_count, ep_reward = 0, 0, 0.\n",
    "        total_reward, self.total_loss, self.total_q = 0., 0., 0.\n",
    "        max_avg_ep_reward = 0\n",
    "        ep_rewards, actions = [], []\n",
    "        \n",
    "        # New game (we could modify this to start randomly -> taking random initial actions)\n",
    "        screen = self.env.new_game()\n",
    "        \n",
    "        # Stacking the screen in the first input buffer\n",
    "        for _ in range(history_length):\n",
    "            self.history.add(screen)\n",
    "            \n",
    "        # Training:\n",
    "        for self.step in tqdm(range(start_step,max_steps),ncols=70,initial=start_step):\n",
    "            if self.step == learn_start:\n",
    "                num_game, self.update_count, ep_reward = 0, 0, 0.\n",
    "                total_reward, self.total_loss, self.total_q = 0., 0., 0.\n",
    "                ep_rewards, actions = [], []\n",
    "                \n",
    "            # 1. Predict: Use our training network to select an action\n",
    "            action = self.predict(self.history.get())\n",
    "            \n",
    "            # 2. Execute the action\n",
    "            screen, reward, terminal = self.env.execute_action(action,is_training=True)\n",
    "            \n",
    "            # 3. New observation\n",
    "            self.observe(screen,reward,action,terminal)\n",
    "            \n",
    "            if terminal:\n",
    "                screen = self.env.new_game()\n",
    "                num_game += 1\n",
    "                ep_rewards.append(ep_reward)\n",
    "                ep_reward = 0\n",
    "            else:\n",
    "                ep_reward += reward\n",
    "                total_reward += reward\n",
    "                actions.append(action)\n",
    "                \n",
    "            actions.append(action)\n",
    "            \n",
    "            if self.step >= learn_start:\n",
    "                if self.step % test_step == test_step - 1:\n",
    "                    avg_reward = total_reward / test_step\n",
    "                    avg_loss = self.total_loss / self.update_count\n",
    "                    avg_q = self.total_q / self.update_count\n",
    "                    \n",
    "                    try:\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        max_ep_reward, min_ep_reward, avg_ep_reward = 0,0,0\n",
    "                        \n",
    "                    print('////////////////////////\\nAverage reward: %.4f \\nAverage loss: %.6f \\nAverage Q: %3.6f'\\\n",
    "                         % (avg_reward, avg_loss, avg_q))\n",
    "                    print('\\nAvg. Ep. Reward: %.4f \\nMax Ep. Reward: %.4f \\nMin Ep. Reward: %.4f \\n# Game: %d'\\\n",
    "                         % (avg_ep_reward, max_ep_reward, min_ep_reward, num_game))\n",
    "                    \n",
    "                    if max_avg_ep_reward * 0.9 <= avg_ep_reward:\n",
    "                        self.step_assign_op.eval({self.step_input: self.step+1})\n",
    "                        self.save_model(self.step+1)\n",
    "                        max_avg_ep_reward = max(max_avg_ep_reward, avg_ep_reward)\n",
    "                        \n",
    "                    if self.step > 180:\n",
    "                        self.inject_summary({\n",
    "                                'average.reward' : avg_reward,\n",
    "                                'average.loss': avg_loss,\n",
    "                                'average.q': avg_q,\n",
    "                                'episode.max reward':max_ep_reward,\n",
    "                                'episode.min reward':min_ep_reward,\n",
    "                                'episode.avg reward':avg_ep_reward,\n",
    "                                'episode.num of game':num_game,\n",
    "                                'episode.rewards': ep_rewards,\n",
    "                                'episode.actions': actions,\n",
    "                                'training.learning_rate': self.learning_rate_op.eval({self.learning_rate_step:self.step})\n",
    "                            })\n",
    "                        \n",
    "                    num_game = 0\n",
    "                    total_reward = 0.\n",
    "                    self.total_loss = 0.\n",
    "                    self.total_q = 0.\n",
    "                    self.update_count = 0\n",
    "                    ep_reward = 0\n",
    "                    ep_rewards = []       \n",
    "                    actions = []\n",
    "            \n",
    "    def predict(self,current_state,test_ep=None):\n",
    "        # Calculate exploration prob. epsilon => This is a decaying epsilon starting at 1 and decreasing until 0.1\n",
    "        # once the learning process start\n",
    "        ep = test_ep or (ep_min + max(0.,(ep_start-ep_min)*(ep_end_time-max(0.,self.step-learn_start))/ep_end_time))\n",
    "        if random.random() < ep:\n",
    "            # Explore: random action\n",
    "            action = random.randrange(self.env.action_size)\n",
    "        else:\n",
    "            action = self.q_action.eval({self.input_state:[current_state]})[0]\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def observe(self,screen,reward,action,terminal):\n",
    "        # Clip reward\n",
    "        reward = max(min_reward,min(max_reward,reward))\n",
    "        \n",
    "        # Add to history\n",
    "        self.history.add(screen)\n",
    "        # Add to exp. replay\n",
    "        self.exp_replay.add(screen,reward,action,terminal)\n",
    "        \n",
    "        if self.step > learn_start:\n",
    "            # If it is time to train the network\n",
    "            if self.step % train_frequency == 0:\n",
    "                self.q_learning_mini_batch()\n",
    "                \n",
    "            # If it is time to update Target network\n",
    "            if self.step % target_q_update_step == target_q_update_step -1:\n",
    "                self.update_target_q_network()\n",
    "        \n",
    "    def q_learning_mini_batch(self):\n",
    "        if self.exp_replay.count < history_length:\n",
    "            # Not enough experiences\n",
    "            return\n",
    "        else:\n",
    "            init_state, action, reward, end_state, terminal = self.exp_replay.sample_from_replay()\n",
    "            \n",
    "            # Standard DQN implementation\n",
    "            # Get the Q-value of the next state\n",
    "            q_t_plus_1 = self.target_q.eval({self.target_state: end_state})\n",
    "            terminal = np.array(terminal) + 0.\n",
    "            # Get max Q_t+1\n",
    "            max_q_t_plus_1 = np.max(q_t_plus_1, axis=1)\n",
    "            # The target q-value (if is not terminal state) will be:\n",
    "            target_q_t = (1. - terminal) * gamma * max_q_t_plus_1 + reward\n",
    "\n",
    "            _, q_t, loss, summary_str = self.sess.run([self.optim,self.q,self.loss,self.q_summary], {\n",
    "                    self.target_q_t: target_q_t,\n",
    "                    self.action: action,\n",
    "                    self.input_state: init_state,\n",
    "                    self.learning_rate_step: self.step\n",
    "                })\n",
    "\n",
    "            self.writer.add_summary(summary_str,self.step)\n",
    "            self.total_loss += loss\n",
    "            self.total_q += q_t.mean()\n",
    "            self.update_count += 1      \n",
    "            \n",
    "    def inject_summary(self,tag_dir):\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dir.keys()], {\n",
    "                self.summary_placeholders[tag]: value for tag , value in tag_dir.items()\n",
    "            })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str,self.step)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Main code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-29 15:17:23,266] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Checkpoints...\n",
      "[*] Load succesfull: Models/-100000\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                   | 149994/49900000 [17:31<284:28:35, 48.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////////////////////\n",
      "Average reward: 0.0228 \n",
      "Average loss: 0.140326 \n",
      "Average Q: 0.340268\n",
      "\n",
      "Avg. Ep. Reward: 0.2996 \n",
      "Max Ep. Reward: 5.0000 \n",
      "Min Ep. Reward: 0.0000 \n",
      "# Game: 3809\n",
      "[*] Saving checkpoints...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'avergae.loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c9a0c62b5104>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGymEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Breakout-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-079d9f9cd719>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m                                 \u001b[1;34m'episode.rewards'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mep_rewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                                 \u001b[1;34m'episode.actions'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                                 \u001b[1;34m'training.learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                             })\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-079d9f9cd719>\u001b[0m in \u001b[0;36minject_summary\u001b[1;34m(self, tag_dir)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minject_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dir.keys()], {\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_placeholders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtag_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             })\n",
      "\u001b[1;31mKeyError\u001b[0m: 'avergae.loss'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "  0%|                   | 149994/49900000 [17:50<98:40:06, 140.06it/s]"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = GymEnvironment('Breakout-v0')\n",
    "    agent = Agent(env,sess)\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train = True\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "gpu_config.log_device_placement = True\n",
    "\n",
    "with tf.Session(config=gpu_config) as sess:\n",
    "    env = GymEnvironment('Breakout-v0')\n",
    "    agent = Agent(env,sess)\n",
    "    \n",
    "    if is_train:\n",
    "        agent.train()\n",
    "    else:\n",
    "        agent.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
