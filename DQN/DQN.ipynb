{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from inspect import getsourcefile\n",
    "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
    "import_path = os.path.abspath(os.path.join(current_path, \"..\"))\n",
    "\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "import commonOps as cops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'num_episodes': 100000,\n",
    "    \n",
    "    'eval_freq': 1000,\n",
    "    'eval_steps': 20,\n",
    "    \n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    'pool_frame_size': 1,\n",
    "    \n",
    "    'memory_size': 1000000,       # Replay memory size\n",
    "    'batch_size': 32,             # Number of training cases ove which SGD update is computed\n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "         \n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'ep_min': 0.1,                 # Final exploration\n",
    "    'ep_start': 1.0,               # Initial exploration\n",
    "    'exploration_steps': 250000,   # Final exploration frame\n",
    " \n",
    "    'target_q_update_step': 10000, # Target network update frequency\n",
    "    'log_online_summary_rate': 100,\n",
    "    'steps_before_training': 12500,   \n",
    "    'save_rate': 1000,\n",
    "    'update_summary_rate': 50000,\n",
    "    'sync_rate': 2500,\n",
    "    \n",
    "    # Clip rewards\n",
    "    'min_reward': -1.0,\n",
    "    'max_reward': 1.0,\n",
    "\n",
    "    # How many times should the same action be taken\n",
    "    'action_repeat': 1,\n",
    "    \n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    'device': '/gpu:0'\n",
    "}\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GameScreen:\n",
    "    def __init__(self, conf):\n",
    "        self.screens = np.zeros([conf['pool_frame_size'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.screens[:-1] = self.screens[1:]\n",
    "        self.screens[-1] = screen\n",
    "        \n",
    "    def get(self):        \n",
    "        return np.amax(np.transpose(self.screens,(1,2,0)),axis=2)\n",
    "    \n",
    "class GymEnvironment():\n",
    "    def __init__(self,name,conf):\n",
    "        self.env = gym.make(name)            # Initialize Gym environment\n",
    "        self.game_screen = GameScreen(conf)\n",
    "        self.screen_width = conf['screen_width']         \n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.random_start = conf['random_start']\n",
    "        self.action_repeat = conf['action_repeat']\n",
    "        self.pool_frame_size = conf['pool_frame_size']\n",
    "        self.display = False\n",
    "    \n",
    "    def set_display(self,value=False):\n",
    "        self.display = value\n",
    "        \n",
    "    def new_game(self):\n",
    "        self._observation = self.env.reset()\n",
    "        \n",
    "        for i in range(self.pool_frame_size):\n",
    "            self.game_screen.add(self.observation)\n",
    "            \n",
    "        self.render()        \n",
    "        return self.screen\n",
    "    \n",
    "    def new_random_game(self):  # Starts a random new game doing \n",
    "        _ = self.new_game()\n",
    "        terminal = False\n",
    "        for _ in xrange(random.randint(0,self.random_start-1)):\n",
    "            self._observation, reward, terminal, _ = self.env.step(0)\n",
    "            self.game_screen.add(self.observation)\n",
    "        \n",
    "        self.render()\n",
    "        return self.screen, 0, terminal\n",
    "        \n",
    "    def execute_action(self,action,is_training=True):\n",
    "        # This function execute the selected action for 'action_repeat' number of times and returns the cumulative reward\n",
    "        # and final state\n",
    "        cum_reward = 0\n",
    "        start_lives = self.num_lives\n",
    "        \n",
    "        for _ in xrange(self.action_repeat):\n",
    "            self._observation, reward, terminal, _ = self.env.step(action)\n",
    "            self.game_screen.add(self.observation)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            if is_training and start_lives > self.num_lives:\n",
    "                terminal = True\n",
    "                \n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        self.render()\n",
    "        \n",
    "        return self.screen, cum_reward, terminal\n",
    "    \n",
    "    @property\n",
    "    def screen(self):\n",
    "        return self.game_screen.get()\n",
    "        \n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.env.action_space.n        # Number of available actions\n",
    "\n",
    "    @property\n",
    "    def num_lives(self):\n",
    "        return self.env.ale.lives()\n",
    "    \n",
    "    @property\n",
    "    def observation(self):     # Method to resize the screen provided by gym to the desired values\n",
    "        return cv2.resize(cv2.cvtColor(self._observation,cv2.COLOR_RGB2GRAY)/255.,(self.screen_width,self.screen_height))\n",
    "    \n",
    "    def render(self):    # Renders the environment only if display == True\n",
    "        if self.display:\n",
    "            self.env.render()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experience_Replay():\n",
    "    def __init__(self,conf):\n",
    "        self.memory_size = conf['memory_size']\n",
    "        # These are the arrays where we will store the experiences\n",
    "        self._experience = {}\n",
    "        self.batch_size = conf['batch_size']       \n",
    "        self.current = 0     # Pointer to the current saving location\n",
    "        self.count = 0       # Number of collected experiences\n",
    "        \n",
    "    def add(self, experience):\n",
    "        '''\n",
    "        Stores experience: experience is a tuple of (s1,a,r,s2,t)\n",
    "        '''        \n",
    "        if self.current in self._experience:\n",
    "            del self._experience[self.current]\n",
    "        self._experience[self.current] = experience\n",
    "        \n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "                    \n",
    "    def retrieve(self,indices):\n",
    "        '''\n",
    "        Get experiences from indices\n",
    "        '''\n",
    "        return [self._experience[v] for v in indices]\n",
    "        \n",
    "    def sample_from_replay(self):\n",
    "        # Randomly sampling from experiences (Standard implementation)\n",
    "        indexes = []\n",
    "        while len(indexes) < self.batch_size:\n",
    "            index = random.randint(0,self.count-1)\n",
    "            indexes.append(index)\n",
    "            \n",
    "        experience_batch = self.retrieve(indexes)\n",
    "        state, action , reward, next_state, terminal = map(np.array, zip(*experience_batch))\n",
    "        \n",
    "        return state, action, reward, next_state, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, conf):\n",
    "        self.history = np.zeros([conf['history_length'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = screen\n",
    "        \n",
    "    def get(self):\n",
    "        return np.transpose(self.history,(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, config, session, num_actions):\n",
    "        self.config = config\n",
    "        self.sess = session\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.gamma = config['gamma']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        self.exp_replay = Experience_Replay(self.config)\n",
    "        self.history = History(self.config)\n",
    "        \n",
    "        self.update_thread = threading.Thread(target=lambda: 0)\n",
    "        self.update_thread.start()\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.episode = 0\n",
    "        self.isTesting = False\n",
    "        \n",
    "        self.reset_game()\n",
    "        self.timeout_option = tf.RunOptions(timeout_in_ms=5000)\n",
    "        \n",
    "        # build the net\n",
    "        with tf.device(config['device']):\n",
    "            # Create all variables \n",
    "            self.state_ph = tf.placeholder(tf.float32, [None, config['screen_width'], config['screen_height'], config['history_length']], name='state_ph')\n",
    "            self.stateT_ph = tf.placeholder(tf.float32, [None, config['screen_width'], config['screen_height'], config['history_length']], name='stateT_ph')\n",
    "            self.action_ph = tf.placeholder(tf.int64, [None], name='action_ph')\n",
    "            self.reward_ph = tf.placeholder(tf.float32, [None], name='reward_ph')\n",
    "            self.terminal_ph = tf.placeholder(tf.float32, [None], name='terminal_ph')\n",
    "            # Define training network\n",
    "            with tf.variable_scope('Q'):\n",
    "                self.Q = self.Q_network(self.state_ph, config, 'Normal')\n",
    "            # Define Target network\n",
    "            with tf.variable_scope('QT'):\n",
    "                self.QT = self.Q_network(self.stateT_ph, config, 'Target')\n",
    "            \n",
    "            # Define training operation\n",
    "            self.train_op = self.train_op(self.Q, self.QT, self.action_ph, self.reward_ph, self.terminal_ph, config, 'Normal')\n",
    "            \n",
    "            # Define operation to copy parameteres from training to target net.\n",
    "            with tf.variable_scope('Copy_parameters'):\n",
    "                self.sync_QT_op = []\n",
    "                for W_pair in zip(tf.get_collection('Target_weights'),tf.get_collection('Normal_weights')):\n",
    "                    self.sync_QT_op.append(W_pair[0].assign(W_pair[1]))\n",
    "                \n",
    "            # Define the summary ops\n",
    "            self.Q_summary_op = tf.merge_summary(tf.get_collection('Normal_summaries'))\n",
    "\n",
    "        self.summary_writter = tf.train.SummaryWriter(config['log_dir'], self.sess.graph, flush_secs=20)\n",
    "\n",
    "    def update(self):\n",
    "        init_state, action, reward, next_state, terminal = self.exp_replay.sample_from_replay()\n",
    "    \n",
    "        feed_dict={self.state_ph: init_state,\n",
    "                    self.stateT_ph: next_state,\n",
    "                    self.action_ph: action,\n",
    "                    self.reward_ph: reward,\n",
    "                    self.terminal_ph: terminal}\n",
    "        if self.step_count % self.config['update_summary_rate'] == 0:\n",
    "            _, Q_summary_str = self.sess.run([self.train_op, self.Q_summary_op], feed_dict, options=self.timeout_option)\n",
    "            self.summary_writter.add_summary(Q_summary_str, self.step_count)\n",
    "        else:\n",
    "            _ = self.sess.run(self.train_op, feed_dict, options=self.timeout_option)\n",
    "\n",
    "        if self.step_count % self.config['sync_rate'] == 0:\n",
    "            self.sess.run(self.sync_QT_op)\n",
    "\n",
    "    def Q_network(self, input_state, config, Collection=None):\n",
    "        conv_stack_shape=[(32,8,4),\n",
    "                    (64,4,2),\n",
    "                    (64,3,1)]\n",
    "\n",
    "        head = cops.conv_stack(input_state, conv_stack_shape, Collection)\n",
    "        head = cops.flatten(head)\n",
    "        head = cops.add_relu_layer(head, size=512, Collection=Collection)\n",
    "        Q = cops.add_linear_layer(head, self.num_actions, Collection, layer_name=\"Q\")\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def train_op(self, Q, QT, action, reward, terminal, config, Collection):\n",
    "        with tf.name_scope('Loss'):\n",
    "            action_one_hot = tf.one_hot(action, self.num_actions, 1., 0., name='action_one_hot')\n",
    "            acted_Q = tf.reduce_sum(Q * action_one_hot, reduction_indices=1, name='DQN_acted')\n",
    "\n",
    "            QT_max_action = tf.reduce_max(QT, 1)\n",
    "            Y = reward + self.gamma * QT_max_action * (1 - terminal)\n",
    "            Y = tf.stop_gradient(Y)\n",
    "\n",
    "            loss_batch = cops.clipped_l2(Y, acted_Q)\n",
    "            loss = tf.reduce_sum(loss_batch, name='loss')\n",
    "\n",
    "            tf.scalar_summary('losses/loss', loss, collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('losses/loss_0', loss_batch[0],collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('losses/loss_max', tf.reduce_max(loss_batch),collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/Y_0', Y[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/Y_max', tf.reduce_max(Y), collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/QT_max_action_0', QT_max_action[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/acted_Q_0', acted_Q[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/acted_Q_max', tf.reduce_max(acted_Q), collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/reward_max', tf.reduce_max(reward), collections=[Collection + '_summaries'])\n",
    "\n",
    "        train_op, grads = cops.graves_rmsprop_optimizer(loss, self.learning_rate, 0.95, 0.01, 1)\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def testing(self, t=True):\n",
    "        self.isTesting = t\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.history.history.fill(0)\n",
    "\n",
    "    def epsilon(self):\n",
    "        if self.step_count < self.config['exploration_steps']:\n",
    "            return self.config['ep_start'] - ((self.config['ep_start'] - self.config['ep_min']) / self.config['exploration_steps']) * self.step_count\n",
    "        else:\n",
    "            return self.config['ep_min']\n",
    "\n",
    "    def e_greedy_action(self, epsilon):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(self.sess.run(self.Q, feed_dict={self.state_ph: [self.history.get()]})[0])\n",
    "        return action\n",
    "    \n",
    "    def observe(self,screen,r,terminal):\n",
    "        if not self.isTesting:  \n",
    "            r = max(self.config['min_reward'], min(self.config['max_reward'], r))\n",
    "            prev_state = self.history.get()\n",
    "            # Add to history\n",
    "            self.history.add(screen)\n",
    "            # Add to Exp replay\n",
    "            self.exp_replay.add((prev_state,r,self.game_action,self.history.get(),terminal))\n",
    "        else:\n",
    "            self.history.add(screen)\n",
    "    \n",
    "    def select_action(self):       \n",
    "        if not self.isTesting:            \n",
    "            self.game_action = self.e_greedy_action(self.epsilon())\n",
    "            if self.step_count > self.config['steps_before_training']:\n",
    "                self.update_thread.join()\n",
    "                self.update_thread = threading.Thread(target=self.update)\n",
    "                self.update_thread.start()\n",
    "            self.step_count += 1\n",
    "        else:\n",
    "            self.game_action = self.e_greedy_action(0.01)\n",
    "        return self.game_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_run(agent, env, n):\n",
    "    agent.testing(True)\n",
    "    score_list = []\n",
    "    for episode in range(n):\n",
    "        screen, r, terminal, score = env.new_game(), 0, False, 0\n",
    "        agent.history.add(screen)\n",
    "        \n",
    "        while not terminal:\n",
    "            action = agent.select_action()\n",
    "            screen, r, terminal = env.execute_action(action,is_training=False)\n",
    "            agent.observe(screen,r,terminal)\n",
    "            score += r\n",
    "        agent.reset_game()\n",
    "        score_list.append(score)\n",
    "    agent.testing(False)\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = GymEnvironment('Breakout-v0',conf_parameters)\n",
    "num_actions = env.action_size\n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.allow_soft_placement = True\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess_config.log_device_placement = False\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    agent = Agent(conf_parameters, sess, num_actions)\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for episode in tqdm(range(conf_parameters['num_episodes']),ncols=70,initial=0):  \n",
    "    #for episode in range(conf_parameters['num_episodes']):\n",
    "        screen, r, terminal, score = env.new_game(), 0, False, 0\n",
    "        agent.history.add(screen)\n",
    "        \n",
    "        ep_begin_t = time.time()\n",
    "        ep_begin_step_count = agent.step_count\n",
    "        while not terminal:\n",
    "            action = agent.select_action()\n",
    "            screen, r, terminal = env.execute_action(action,is_training=False)\n",
    "            agent.observe(screen,r,terminal)\n",
    "            score += r\n",
    "        agent.reset_game()\n",
    "        ep_duration = time.time() - ep_begin_t\n",
    "\n",
    "        is_final_episode = conf_parameters['num_episodes'] == episode\n",
    "        # online Summary\n",
    "        if episode % conf_parameters['log_online_summary_rate'] == 0 or is_final_episode:\n",
    "            episode_online_summary = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/epsilon\",\n",
    "                        simple_value=agent.epsilon()),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/online\",\n",
    "                        simple_value=score),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/global_step\",\n",
    "                        simple_value=agent.step_count),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/step_duration\",\n",
    "                        simple_value=ep_duration/(agent.step_count - ep_begin_step_count)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/ep_duration_seconds\",\n",
    "                        simple_value=ep_duration)])\n",
    "            agent.summary_writter.add_summary(episode_online_summary, episode)\n",
    "\n",
    "        # save\n",
    "        if (episode % conf_parameters['save_rate'] == 0 and episode != 0) or is_final_episode:\n",
    "            saver.save(sess,os.path.join(conf_parameters['checkpoint_dir'],'model'), global_step=episode) \n",
    "            \n",
    "        # performance summary\n",
    "        if episode % conf_parameters['eval_freq'] == 0 or is_final_episode:\n",
    "            score_list = test_run(agent, env, conf_parameters['eval_steps'])\n",
    "            performance_summary = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/average\",simple_value=sum(score_list)/len(score_list)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/max\",simple_value=max(score_list)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/min\",simple_value=min(score_list)),\n",
    "                ])\n",
    "            agent.summary_writter.add_summary(performance_summary, agent.step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
