{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from inspect import getsourcefile\n",
    "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
    "import_path = os.path.abspath(os.path.join(current_path, \"..\"))\n",
    "\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "import commonOps as cops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'num_episodes': 100000,\n",
    "    \n",
    "    'eval_freq': 1000,\n",
    "    'eval_steps': 20,\n",
    "    \n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    'pool_frame_size': 1,\n",
    "    \n",
    "    'memory_size': 1000000,       # Replay memory size\n",
    "    'batch_size': 32,             # Number of training cases ove which SGD update is computed\n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "         \n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'ep_min': 0.1,                 # Final exploration\n",
    "    'ep_start': 1.0,               # Initial exploration\n",
    "    'exploration_steps': 250000,   # Final exploration frame\n",
    " \n",
    "    'target_q_update_step': 10000, # Target network update frequency\n",
    "    'log_online_summary_rate': 100,\n",
    "    'steps_before_training': 12500,   \n",
    "    'save_rate': 1000,\n",
    "    'update_summary_rate': 50000,\n",
    "    'sync_rate': 2500,\n",
    "    \n",
    "    # Clip rewards\n",
    "    'min_reward': -1.0,\n",
    "    'max_reward': 1.0,\n",
    "\n",
    "    # How many times should the same action be taken\n",
    "    'action_repeat': 1,\n",
    "    \n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    'device': '/gpu:0'\n",
    "}\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, config):\n",
    "        self.capacity = self.memory_size = config['memory_size']\n",
    "        self.batch_size = config['batch_size']    \n",
    "        self.buff_size = config['history_length']\n",
    "        self.screens = np.empty((self.capacity, config['screen_width'],config['screen_height']), dtype=np.uint8)\n",
    "        self.actions = np.empty((self.capacity), dtype=np.uint8)\n",
    "        self.rewards = np.empty((self.capacity), dtype=np.int8)\n",
    "        self.terminals = np.empty((self.capacity), dtype=np.bool)\n",
    "        self.next_state_batch = np.empty((self.batch_size, config['screen_width'],config['screen_height'], self.buff_size), dtype=np.uint8)\n",
    "        self.state_batch = np.empty((self.batch_size, config['screen_width'],config['screen_height'], self.buff_size), dtype=np.uint8)\n",
    "        self.current = 0\n",
    "        self.step = 0\n",
    "        self.filled = False\n",
    "\n",
    "    def add(self, screen, action, reward, terminal):\n",
    "        self.screens[self.current] = screen\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminals[self.current] = terminal\n",
    "        self.current += 1\n",
    "        self.step += 1\n",
    "        if self.current == self.capacity:\n",
    "            self.current = 0\n",
    "            self.filled = True\n",
    "\n",
    "    def get_state(self, index):\n",
    "        if self.filled == False:\n",
    "            assert index < self.current, \"%i index has note been added yet\"%index\n",
    "        #Fast slice read\n",
    "        if index >= self.buff_size - 1:\n",
    "            state = self.screens[(index - self.buff_size+1):(index + 1), ...]\n",
    "        #Slow list read\n",
    "        else:\n",
    "            indexes = [(index - i) % self.capacity for i in reversed(range(self.buff_size))]\n",
    "            state = self.screens[indexes, ...]\n",
    "        # different screens should be in the 3rd dim as channels\n",
    "        return np.transpose(state, [1,2,0])\n",
    "\n",
    "    def sample_transition_batch(self):\n",
    "        if self.filled == False:\n",
    "            assert self.current >= self.batch_size, \"There is not enough to sample. Call add bathc_size times\"\n",
    "        indexes = []\n",
    "        while len(indexes) != self.batch_size:\n",
    "            # index, is the index of state, and index + 1 of next_state\n",
    "            if self.filled:\n",
    "                index = random.randint(0, self.capacity - 2) # -2 because index +1 will be used for next state\n",
    "                # if index is in the space we are currently writing\n",
    "                if index >= self.current and index - self.buff_size < self.current:\n",
    "                    continue\n",
    "            else:\n",
    "                # can't start from 0 because get_state would loop back to the end -- wich is uninitialized.\n",
    "                # index +1 can be terminal\n",
    "                index = random.randint(self.buff_size -1, self.current -2)\n",
    "\n",
    "            # We check that current state is not terminal\n",
    "            if self.terminals[(index - self.buff_size + 1):index+1].any():\n",
    "                continue\n",
    "            self.state_batch[len(indexes)] = self.get_state(index)\n",
    "            self.next_state_batch[len(indexes)] = self.get_state(index + 1)\n",
    "            indexes.append(index)\n",
    "        action_batch = self.actions[indexes]\n",
    "        reward_batch = self.rewards[indexes]\n",
    "        terminal_batch = self.terminals[indexes]\n",
    "        return self.state_batch, action_batch, reward_batch, self.next_state_batch, terminal_batch, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, config, session, num_actions):\n",
    "        self.config = config\n",
    "        self.sess = session\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.gamma = config['gamma']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        self.exp_replay = ReplayMemory(self.config)\n",
    "        self.game_state = np.zeros((1, config['screen_width'], config['screen_height'], config['history_length']), dtype=np.uint8)\n",
    "        \n",
    "        self.update_thread = threading.Thread(target=lambda: 0)\n",
    "        self.update_thread.start()\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.episode = 0\n",
    "        self.isTesting = False\n",
    "        \n",
    "        self.reset_game()\n",
    "        self.timeout_option = tf.RunOptions(timeout_in_ms=5000)\n",
    "        \n",
    "        # build the net\n",
    "        with tf.device(config['device']):\n",
    "            # Create all variables \n",
    "            self.state_ph = tf.placeholder(tf.float32, [None, config['screen_width'], config['screen_height'], config['history_length']], name='state_ph')\n",
    "            self.stateT_ph = tf.placeholder(tf.float32, [None, config['screen_width'], config['screen_height'], config['history_length']], name='stateT_ph')\n",
    "            self.action_ph = tf.placeholder(tf.int64, [None], name='action_ph')\n",
    "            self.reward_ph = tf.placeholder(tf.float32, [None], name='reward_ph')\n",
    "            self.terminal_ph = tf.placeholder(tf.float32, [None], name='terminal_ph')\n",
    "            # Define training network\n",
    "            with tf.variable_scope('Q'):\n",
    "                self.Q = self.Q_network(self.state_ph, config, 'Normal')\n",
    "            # Define Target network\n",
    "            with tf.variable_scope('QT'):\n",
    "                self.QT = self.Q_network(self.stateT_ph, config, 'Target')\n",
    "            \n",
    "            # Define training operation\n",
    "            self.train_op = self.train_op(self.Q, self.QT, self.action_ph, self.reward_ph, self.terminal_ph, config, 'Normal')\n",
    "            \n",
    "            # Define operation to copy parameteres from training to target net.\n",
    "            with tf.variable_scope('Copy_parameters'):\n",
    "                self.sync_QT_op = []\n",
    "                for W_pair in zip(tf.get_collection('Target_weights'),tf.get_collection('Normal_weights')):\n",
    "                    self.sync_QT_op.append(W_pair[0].assign(W_pair[1]))\n",
    "                \n",
    "            # Define the summary ops\n",
    "            self.Q_summary_op = tf.merge_summary(tf.get_collection('Normal_summaries'))\n",
    "\n",
    "        self.summary_writter = tf.train.SummaryWriter(config['log_dir'], self.sess.graph, flush_secs=20)\n",
    "\n",
    "    def update(self):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = self.exp_replay.sample_transition_batch()\n",
    "    \n",
    "        feed_dict={self.state_ph: state_batch,\n",
    "                    self.stateT_ph: next_state_batch,\n",
    "                    self.action_ph: action_batch,\n",
    "                    self.reward_ph: reward_batch,\n",
    "                    self.terminal_ph: terminal_batch}\n",
    "        if self.step_count % self.config['update_summary_rate'] == 0:\n",
    "            _, Q_summary_str = self.sess.run([self.train_op, self.Q_summary_op], feed_dict, options=self.timeout_option)\n",
    "            self.summary_writter.add_summary(Q_summary_str, self.step_count)\n",
    "        else:\n",
    "            _ = self.sess.run(self.train_op, feed_dict, options=self.timeout_option)\n",
    "\n",
    "        if self.step_count % self.config['sync_rate'] == 0:\n",
    "            self.sess.run(self.sync_QT_op)\n",
    "\n",
    "    def Q_network(self, input_state, config, Collection=None):\n",
    "        conv_stack_shape=[(32,8,4),\n",
    "                    (64,4,2),\n",
    "                    (64,3,1)]\n",
    "\n",
    "        head = cops.conv_stack(input_state, conv_stack_shape, Collection)\n",
    "        head = cops.flatten(head)\n",
    "        head = cops.add_relu_layer(head, size=512, Collection=Collection)\n",
    "        Q = cops.add_linear_layer(head, self.num_actions, Collection, layer_name=\"Q\")\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def train_op(self, Q, QT, action, reward, terminal, config, Collection):\n",
    "        with tf.name_scope('Loss'):\n",
    "            action_one_hot = tf.one_hot(action, self.num_actions, 1., 0., name='action_one_hot')\n",
    "            acted_Q = tf.reduce_sum(Q * action_one_hot, reduction_indices=1, name='DQN_acted')\n",
    "\n",
    "            QT_max_action = tf.reduce_max(QT, 1)\n",
    "            Y = reward + self.gamma * QT_max_action * (1 - terminal)\n",
    "            Y = tf.stop_gradient(Y)\n",
    "\n",
    "            loss_batch = cops.clipped_l2(Y, acted_Q)\n",
    "            loss = tf.reduce_sum(loss_batch, name='loss')\n",
    "\n",
    "            tf.scalar_summary('losses/loss', loss, collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('losses/loss_0', loss_batch[0],collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('losses/loss_max', tf.reduce_max(loss_batch),collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/Y_0', Y[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/Y_max', tf.reduce_max(Y), collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/QT_max_action_0', QT_max_action[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/acted_Q_0', acted_Q[0], collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/acted_Q_max', tf.reduce_max(acted_Q), collections=[Collection + '_summaries'])\n",
    "            tf.scalar_summary('main/reward_max', tf.reduce_max(reward), collections=[Collection + '_summaries'])\n",
    "\n",
    "        train_op, grads = cops.graves_rmsprop_optimizer(loss, self.learning_rate, 0.95, 0.01, 1)\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def testing(self, t=True):\n",
    "        self.isTesting = t\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.episode_begining = True\n",
    "        self.game_state.fill(0)\n",
    "\n",
    "    def epsilon(self):\n",
    "        if self.step_count < self.config['exploration_steps']:\n",
    "            return self.config['ep_start'] - ((self.config['ep_start'] - self.config['ep_min']) / self.config['exploration_steps']) * self.step_count\n",
    "        else:\n",
    "            return self.config['ep_min']\n",
    "\n",
    "    def e_greedy_action(self, epsilon):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(self.sess.run(self.Q, feed_dict={self.state_ph: self.game_state})[0])\n",
    "        return action\n",
    "    \n",
    "    def done(self):\n",
    "        if not self.isTesting:\n",
    "            self.exp_replay.add(self.game_state[:, :, :, -1],self.game_action, self.game_reward, True)\n",
    "        self.reset_game()\n",
    "\n",
    "    def observe(self, x, r):\n",
    "        self.game_reward = r\n",
    "        x_ = cv2.resize(x, (self.config['screen_width'], self.config['screen_height']))\n",
    "        x_ = cv2.cvtColor(x_, cv2.COLOR_RGB2GRAY)\n",
    "        self.game_state = np.roll(self.game_state, -1, axis=3)\n",
    "        self.game_state[0, :, :, -1] = x_\n",
    "            \n",
    "    def step(self, x, r):\n",
    "        r = max(self.config['min_reward'], min(self.config['max_reward'], r))\n",
    "        if not self.isTesting:\n",
    "            if not self.episode_begining:\n",
    "                self.exp_replay.add(self.game_state[:, :, :, -1], self.game_action, self.game_reward, False)\n",
    "            else:\n",
    "                for i in range(self.config['history_length'] - 1):\n",
    "                    # add the resetted buffer\n",
    "                    self.exp_replay.add(self.game_state[:, :, :, i], 0, 0, False)\n",
    "                self.episode_begining = False\n",
    "            self.observe(x, r)\n",
    "            self.game_action = self.e_greedy_action(self.epsilon())\n",
    "            if self.step_count > self.config['steps_before_training']:\n",
    "                self.update_thread.join()\n",
    "                self.update_thread = threading.Thread(target=self.update)\n",
    "                self.update_thread.start()\n",
    "            self.step_count += 1\n",
    "        else:\n",
    "            self.observe(x, r)\n",
    "            self.game_action = self.e_greedy_action(0.01)\n",
    "        return self.game_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_run(agent, env, n):\n",
    "    agent.testing(True)\n",
    "    score_list = []\n",
    "    for episode in range(n):\n",
    "        screen, r, terminal, score = env.reset(), 0, False, 0\n",
    "        while not terminal:\n",
    "            action = agent.step(screen, r)\n",
    "            screen, r, terminal, _ = env.step(action)\n",
    "            score += r\n",
    "        agent.done()\n",
    "        score_list.append(score)\n",
    "    agent.testing(False)\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "num_actions = env.action_space.n \n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.allow_soft_placement = True\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess_config.log_device_placement = False\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    agent = Agent(conf_parameters, sess, num_actions)\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for episode in tqdm(range(conf_parameters['num_episodes']),ncols=80,initial=0):  \n",
    "        screen, r, terminal, score = env.reset(), 0, False, 0\n",
    "        \n",
    "        ep_begin_t = time.time()\n",
    "        ep_begin_step_count = agent.step_count\n",
    "        while not terminal:\n",
    "            action = agent.step(screen, r)\n",
    "            screen, r, terminal, _ = env.step(action)\n",
    "            score += r\n",
    "        agent.done()\n",
    "        ep_duration = time.time() - ep_begin_t\n",
    "\n",
    "        is_final_episode = conf_parameters['num_episodes'] == episode\n",
    "        # online Summary\n",
    "        if episode % conf_parameters['log_online_summary_rate'] == 0 or is_final_episode:\n",
    "            episode_online_summary = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/epsilon\",\n",
    "                        simple_value=agent.epsilon()),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/online\",\n",
    "                        simple_value=score),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/global_step\",\n",
    "                        simple_value=agent.step_count),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/step_duration\",\n",
    "                        simple_value=ep_duration/(agent.step_count - ep_begin_step_count)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"online/ep_duration_seconds\",\n",
    "                        simple_value=ep_duration)])\n",
    "            agent.summary_writter.add_summary(episode_online_summary, episode)\n",
    "\n",
    "        # save\n",
    "        if (episode % conf_parameters['save_rate'] == 0 and episode != 0) or is_final_episode:\n",
    "            saver.save(sess,os.path.join(conf_parameters['checkpoint_dir'],'model'), global_step=episode) \n",
    "            \n",
    "        # performance summary\n",
    "        if episode % conf_parameters['eval_freq'] == 0 or is_final_episode:\n",
    "            score_list = test_run(agent, env, conf_parameters['eval_steps'])\n",
    "            performance_summary = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/average\",simple_value=sum(score_list)/len(score_list)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/max\",simple_value=max(score_list)),\n",
    "                    tf.Summary.Value(\n",
    "                        tag=\"score/min\",simple_value=min(score_list)),\n",
    "                ])\n",
    "            agent.summary_writter.add_summary(performance_summary, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
