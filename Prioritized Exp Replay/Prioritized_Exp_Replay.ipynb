{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Prioritized Experience Replay\n",
    "\n",
    "Implementing the rank-based approach. Code based on contribution from [Damcy](https://github.com/Damcy/prioritized-experience-replay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from utils import BinaryHeap\n",
    "import gym\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters\n",
    "\n",
    "Configuration parameteres as presented in the [original](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'max_steps': 50000000,\n",
    "    'test_step': 50000,\n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    \n",
    "    'memory_size': 1000000,       # Replay memory size\n",
    "    'batch_size': 32,             # Number of training cases ove which SGD update is computed\n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "    'learn_start': 50000,         # Replay start size\n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'ep_min': 0.1,                 # Final exploration\n",
    "    'ep_start': 1.0,               # Initial exploration\n",
    "    'ep_end_time': 1000000,        # Final exploration frame\n",
    "\n",
    "    'target_q_update_step': 10000, # Target network update frequency\n",
    "\n",
    "    # Train batch\n",
    "    'train_frequency': 4,          # Update frequency\n",
    "    \n",
    "    # Clip rewards\n",
    "    'min_reward': -1.0,\n",
    "    'max_reward': 1.0,\n",
    "\n",
    "    # How many times should the same action be taken\n",
    "    'action_repeat': 4,\n",
    "    \n",
    "    # Prioritized experience replay parameters\n",
    "    'alpha': 0.7,\n",
    "    'beta_init': 0.5,\n",
    "    'part_number': 100\n",
    "}\n",
    "\n",
    "# Whether or not to render the environment \n",
    "display = False\n",
    "\n",
    "checkpoint_dir = 'Models/Prio_Exp/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "log_dir = 'Logs/Prio_Exp/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "log_performance_file = 'Logs/Prio_Exp.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GymEnvironment():\n",
    "    def __init__(self,name,conf):\n",
    "        self.env = gym.make(name)            # Initialize Gym environment\n",
    "        self._screen = None\n",
    "        self.screen_width = conf['screen_width']\n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.random_start = conf['random_start']\n",
    "        \n",
    "    def execute_action(self,action,is_training=True):\n",
    "        # This function execute the selected action for 'action_repeat' number of times and returns the cumulative reward\n",
    "        # and final state\n",
    "        cum_reward = 0\n",
    "        start_lives = self.env.ale.lives()\n",
    "        \n",
    "        for _ in xrange(action_repeat):\n",
    "            self._screen, reward, terminal, _ = self.env.step(action)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            if is_training and start_lives > self.env.ale.lives():\n",
    "                cum_reward -= 1\n",
    "                terminal = True\n",
    "                \n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        reward = cum_reward\n",
    "        self.render()\n",
    "        return self.screen, reward, terminal\n",
    "    \n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.env.action_space.n        # Number of available actions\n",
    "\n",
    "    @property\n",
    "    def screen(self):     # Method to resize the screen provided by gym to the desired values\n",
    "        return cv2.resize(cv2.cvtColor(self._screen,cv2.COLOR_RGB2GRAY)/255.,(self.screen_width,self.screen_height))\n",
    "    \n",
    "    def new_game(self):\n",
    "        if self.env.ale.lives() == 0:\n",
    "            self._screen = self.env.reset()\n",
    "        \n",
    "        self._screen, reward, terminal, _ = self.env.step(0)\n",
    "        self.render()\n",
    "        return self.screen, 0, 0, terminal\n",
    "    \n",
    "    def render(self):    # Renders the environment only if display == True\n",
    "        if display:\n",
    "            self.env.render()\n",
    "    \n",
    "    def new_random_game(self):  # Starts a random new game after doing 'nothing' for a random number of frames\n",
    "        _,_,_,terminal = self.new_game()\n",
    "        for _ in xrange(random.randint(0,self.random_start-1)):\n",
    "            self._screen, reward, terminal, _ = self.env.step(0)\n",
    "        \n",
    "        self.render()\n",
    "        return self.screen, 0, 0, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, conf):\n",
    "        self.history = np.zeros([conf['history_length'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = screen\n",
    "        \n",
    "    def get(self):\n",
    "        return np.transpose(self.history,(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rank_Priority_Replay():\n",
    "    def __init__(self,conf):\n",
    "        self.memory_size = conf['memory_size']\n",
    "        # These are the arrays where we will store the experiences\n",
    "        self._experience = {}\n",
    "        \n",
    "        self.alpha = conf['alpha']\n",
    "        self.beta_init = conf['beta_init']\n",
    "        self.batch_size = conf['batch_size']\n",
    "        self.partition_num = conf['part_number']    # Split total size to N segments\n",
    "        self.learn_start = conf['learn_start']    \n",
    "        self.beta_grad = (1-self.beta_init) / float(conf['max_steps'] - self.learn_start)\n",
    "        \n",
    "        self.priority_queue = BinaryHeap(self.memory_size)\n",
    "        self.distributions = self.build_distributions()        \n",
    "        self.current = 0     # Pointer to the current saving location\n",
    "        self.count = 0       # Number of collected experiences\n",
    "        \n",
    "    def build_distributions(self):\n",
    "        '''\n",
    "        Preprocess probabilities: (rank_i)^(-alpha) / sum((rank_i)^(-alpha))\n",
    "        '''\n",
    "        results = {}  \n",
    "        \n",
    "        # Creating different distributions according to the number of experiences which have been collected\n",
    "        partition_size = int(math.floor(self.memory_size / self.partition_num))\n",
    "        current_partition = 1\n",
    "        \n",
    "        for n in range(partition_size,self.memory_size+1,partition_size):\n",
    "            if self.learn_start <= n:\n",
    "                distribution = {}\n",
    "                # P(i) = (rank_i)^(-alpha) / sum((rank_i)^(-alpha))\n",
    "                pdf = list(map(lambda x: math.pow(x,-self.alpha),range(1,n+1)))\n",
    "                pdf_sum = math.fsum(pdf)\n",
    "                distribution['pdf'] = list(map(lambda x: x/pdf_sum, pdf))\n",
    "                \n",
    "                # Split each distribution to K segments, setting k = batch_size\n",
    "                # strata_ends keeps start and end position of each segment\n",
    "                cdf = np.cumsum(distribution['pdf'])\n",
    "                strata_ends = {1: 0, self.batch_size+1: n}\n",
    "                step = 1/float(self.batch_size)\n",
    "                index = 1\n",
    "                for s in range(2,self.batch_size+1):\n",
    "                    while cdf[index] < step:\n",
    "                        index += 1\n",
    "                    strata_ends[s] = index\n",
    "                    step += 1/float(self.batch_size)\n",
    "                    \n",
    "                distribution['strata_ends'] = strata_ends\n",
    "                results[current_partition] = distribution\n",
    "            \n",
    "            current_partition += 1\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def add(self, experience):\n",
    "        '''\n",
    "        Stores experience: experience is a tuple of (s1,a,r,s2,t)\n",
    "        '''\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        \n",
    "        if self.current in self._experience:\n",
    "            del self._experience[self.current]\n",
    "        self._experience[self.current] = experience\n",
    "\n",
    "        # Add to priority queue\n",
    "        priority = self.priority_queue.get_max_priority()\n",
    "        self.priority_queue.update(priority,self.current)\n",
    "                    \n",
    "    def retrieve(self,indices):\n",
    "        '''\n",
    "        Get experiences from indices\n",
    "        '''\n",
    "        return [self._experience[v] for v in indices]\n",
    "    \n",
    "    def rebalance(self):\n",
    "        '''\n",
    "        Rebalance priority queue\n",
    "        '''\n",
    "        self.priority_queue.balance_tree()\n",
    "        \n",
    "    def update_priority(self,indices,delta):\n",
    "        '''\n",
    "        Update the priority values according to new observations\n",
    "        '''\n",
    "        for i in range(0,len(indices)):\n",
    "            self.priority_queue.update(math.fabs(delta[i]),indices[i])\n",
    "    \n",
    "    def sample_from_priority_replay(self,global_step):\n",
    "        '''\n",
    "        Samples a minibatch from exp replay\n",
    "        '''\n",
    "        if self.count < self.learn_start:\n",
    "            print('Number of records less than learn_start. Failed')\n",
    "            return 0\n",
    "        \n",
    "        # Get distribution according to current number of collected experiences\n",
    "        dist_index = int(math.ceil(self.count / float(self.memory_size) * self.partition_num))\n",
    "        distribution = self.distributions[dist_index]\n",
    "        \n",
    "        local_N = dist_index * math.floor(self.memory_size / self.partition_num)\n",
    "                \n",
    "        rank_list = []\n",
    "        # Sample from K segments, with K = batch_size\n",
    "        n = 1\n",
    "        while len(rank_list) < self.batch_size:\n",
    "            while True:           \n",
    "                # Check that the range is within the number of collected samples\n",
    "                if distribution['strata_ends'][n+1] > self.count:\n",
    "                    # Limit the range to be within available size\n",
    "                    idx = random.randint(distribution['strata_ends'][n]+1, self.count)\n",
    "                else:\n",
    "                    idx = random.randint(distribution['strata_ends'][n]+1, distribution['strata_ends'][n+1])\n",
    "                # Get experience_id corresponding to this index\n",
    "                exp_id = self.priority_queue.priority_to_experience([idx])[0]\n",
    "                # If index wraps over terminal state, get new one\n",
    "                if self._experience[exp_id][4] == True:\n",
    "                    continue\n",
    "                # Otherwise use the index making sure we start from the first interval if necessary\n",
    "                if distribution['strata_ends'][n+1] > self.count:\n",
    "                    n = 1\n",
    "                break\n",
    "            n += 1\n",
    "            rank_list.append(idx)\n",
    "            \n",
    "        # Beta increases linearly from b_0 up to 1\n",
    "        beta = min(self.beta_init + (global_step - self.learn_start - 1) * self.beta_grad, 1)\n",
    "        # Get probabilities P(i)\n",
    "        P_i = [distribution['pdf'][v-1] for v in rank_list]\n",
    "        # Compute the weights\n",
    "        w = np.power(np.array(P_i) * local_N, -beta)\n",
    "        w_max = max(w)\n",
    "        # Normalizing the weights\n",
    "        w = np.divide(w,w_max)\n",
    "        # Rank_list contains priority IDs, convert them to experience IDs\n",
    "        rank_e_id = self.priority_queue.priority_to_experience(rank_list)\n",
    "        # Get experiences\n",
    "        experience_batch = self.retrieve(rank_e_id)\n",
    "        state, action, reward, next_state, terminal = map(np.array, zip(*experience_batch))\n",
    "        \n",
    "        return state, action , reward, next_state, terminal, w, rank_e_id\n",
    "        \n",
    "    def sample_from_replay(self):\n",
    "        # Sample random indexes\n",
    "        indexes = []\n",
    "        while len(indexes) < self.batch_size:\n",
    "            while True:\n",
    "                index = random.randint(1,self.count-1)\n",
    "                # If index wraps over terminal state, get new one\n",
    "                if self._experience[index][4]:\n",
    "                    continue\n",
    "                # Use the index otherwise\n",
    "                break\n",
    "            indexes.append(index)\n",
    "            \n",
    "        experience_batch = self.retrieve(indexes)\n",
    "        state, action , reward, next_state, terminal = map(np.array, zip(*experience_batch))\n",
    "        \n",
    "        return state, action, reward, next_state, terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-16 15:14:41,699] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "env = GymEnvironment('Breakout-v0',conf_parameters)\n",
    "\n",
    "history = History(conf_parameters)\n",
    "screen,_,_,_ = env.new_random_game()\n",
    "        \n",
    "# Stacking the screen in the first input buffer\n",
    "for _ in range(4):\n",
    "    history.add(screen)\n",
    "    \n",
    "replay = Rank_Priority_Replay(conf_parameters)\n",
    "for i in range(50010):\n",
    "    replay.add((history.get(),i,1,history.get(),False))\n",
    "    \n",
    "print('done')\n",
    "\n",
    "#s,a,r,st,t = replay.sample_from_replay()\n",
    "s2,a2,r2,st2,t2,w,ids = replay.sample_from_priority_replay(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
