{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "Solving the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment using reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-nearest neighbours.\n",
    "Part of this code is based on [Andrej Karpathy](https://gym.openai.com/evaluations/eval_lEi8I8v2QLqEgzBxcvRIaA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EpisodicAgent(object):\n",
    "    \"\"\"\n",
    "    Episodic agent is a simple nearest-neighbor based agent:\n",
    "    - At training time it remembers all tuples of (state, action, reward).\n",
    "    - After each episode it computes the empirical value function based \n",
    "        on the recorded rewards in the episode.\n",
    "    - At test time it looks up k-nearest neighbors in the state space \n",
    "        and takes the action that most often leads to highest average value.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        assert isinstance(action_space, gym.spaces.discrete.Discrete), 'unsupported action space for now.'\n",
    "\n",
    "        # options\n",
    "        self.epsilon = 1.0 # probability of choosing a random action\n",
    "        self.epsilon_decay = 0.98 # decay of epsilon per episode\n",
    "        self.epsilon_min = 0\n",
    "        self.nnfind = 500 # how many nearest neighbors to consider in the policy?\n",
    "        self.mem_needed = 500 # amount of data to have before we can start exploiting\n",
    "        self.mem_size = 50000 # maximum size of memory\n",
    "        self.gamma = 0.95 # discount factor\n",
    "\n",
    "        # internal vars\n",
    "        self.iter = 0\n",
    "        self.mem_pointer = 0 # memory pointer\n",
    "        self.max_pointer = 0\n",
    "        self.db = None # large array of states seen\n",
    "        self.dba = {} # actions taken\n",
    "        self.dbr = {} # rewards obtained at all steps\n",
    "        self.dbv = {} # value function at all steps, computed retrospectively\n",
    "        self.ep_start_pointer = 0\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        assert isinstance(observation, np.ndarray) and observation.ndim == 1, 'unsupported observation type for now.'\n",
    "\n",
    "        if self.db is None:\n",
    "            # lazy initialization of memory\n",
    "            self.db = np.zeros((self.mem_size, observation.size))\n",
    "            self.mem_pointer = 0\n",
    "            self.ep_start_pointer = 0\n",
    "\n",
    "        # we have enough data, we want to explore, and we have seen at least one episode already (so values were computed)\n",
    "        if self.iter > self.mem_needed and np.random.rand() > self.epsilon and self.dbv:\n",
    "            # exploit: find the few closest states and pick the action that led to highest rewards\n",
    "            # 1. find k nearest neighbors\n",
    "            ds = np.sum((self.db[:self.max_pointer] - observation)**2, axis=1) # L2 distance\n",
    "            ix = np.argsort(ds) # sorts ascending by distance\n",
    "            ix = ix[:min(len(ix), self.nnfind)] # crop to only some number of nearest neighbors\n",
    "            \n",
    "            # find the action that leads to most success. do a vote among actions\n",
    "            adict = {}\n",
    "            ndict = {}\n",
    "            for i in ix:\n",
    "                vv = self.dbv[i]\n",
    "                aa = self.dba[i]\n",
    "                vnew = adict.get(aa, 0) + vv\n",
    "                adict[aa] = vnew\n",
    "                ndict[aa] = ndict.get(aa, 0) + 1\n",
    "\n",
    "            for a in adict: # normalize by counts\n",
    "                adict[a] = adict[a] / ndict[a]\n",
    "\n",
    "            its = [(y,x) for x,y in adict.iteritems()]\n",
    "            its.sort(reverse=True) # descending\n",
    "            a = its[0][1]\n",
    "\n",
    "        else:\n",
    "            # explore: do something random\n",
    "            a = self.action_space.sample()\n",
    "\n",
    "        # record move to database\n",
    "        if self.mem_pointer < self.mem_size:\n",
    "            self.db[self.mem_pointer] = observation # save the state\n",
    "            self.dba[self.mem_pointer] = a # and the action we took\n",
    "            self.dbr[self.mem_pointer-1] = reward # and the reward we obtained last time step\n",
    "            self.dbv[self.mem_pointer-1] = 0\n",
    "        self.mem_pointer += 1\n",
    "        self.iter += 1\n",
    "\n",
    "        if done: # episode Ended;\n",
    "\n",
    "            # compute the estimate of the value function based on this rollout\n",
    "            v = 0\n",
    "            for t in reversed(xrange(self.ep_start_pointer, self.mem_pointer)):\n",
    "                v = self.gamma * v + self.dbr.get(t,0)\n",
    "                self.dbv[t] = v\n",
    "\n",
    "            self.ep_start_pointer = self.mem_pointer\n",
    "            self.max_pointer = min(max(self.max_pointer, self.mem_pointer), self.mem_size)\n",
    "\n",
    "            # decay exploration probability\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min) # cap at epsilon_min\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-20 17:34:12,922] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 running reward: 0.650000\n",
      "100 running reward: 140.665248\n",
      "200 running reward: 196.045064\n",
      "300 running reward: 198.917342\n",
      "400 running reward: 197.071849\n"
     ]
    }
   ],
   "source": [
    "agent = EpisodicAgent(env.action_space)\n",
    "\n",
    "episode_count = 500\n",
    "max_steps = 200\n",
    "reward = 0\n",
    "done = False\n",
    "sum_reward_running = 0\n",
    "\n",
    "for i in xrange(episode_count):\n",
    "    ob = env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    for j in xrange(max_steps):\n",
    "        action = agent.act(ob, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    sum_reward_running = sum_reward_running * 0.95 + sum_reward * 0.05\n",
    "    if i%100 == 0:\n",
    "        print '%d running reward: %f' % (i, sum_reward_running)\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize performance on new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0\n"
     ]
    }
   ],
   "source": [
    "reward = 0\n",
    "sum_reward = 0\n",
    "done = False\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        action = agent.act(observation, reward, done)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        sum_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(sum_reward)\n",
    "            break\n",
    "    \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
