{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "Solving the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment using reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 17:44:14,132] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using K-nearest neighbours.\n",
    "Based on [Andrej Karpathy's](https://gym.openai.com/evaluations/eval_lEi8I8v2QLqEgzBxcvRIaA) code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nearest-neighbor based agent\n",
    "class EpisodicAgent(object):\n",
    "    \"\"\"\n",
    "    - At training time it remembers all tuples of (state, action, reward).\n",
    "    - After each episode it computes the empirical value function based \n",
    "        on the recorded rewards in the episode.\n",
    "    - At test time it looks up k-nearest neighbors in the state space \n",
    "        and takes the action that most often leads to highest average value.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        assert isinstance(action_space, gym.spaces.discrete.Discrete), 'unsupported action space for now.'\n",
    "\n",
    "        # options\n",
    "        self.epsilon = 1.0 # probability of choosing a random action\n",
    "        self.epsilon_decay = 0.98 # decay of epsilon per episode\n",
    "        self.epsilon_min = 0\n",
    "        self.nnfind = 500 # how many nearest neighbors to consider in the policy?\n",
    "        self.mem_needed = 500 # amount of data to have before we can start exploiting\n",
    "        self.mem_size = 50000 # maximum size of memory\n",
    "        self.gamma = 0.95 # discount factor\n",
    "\n",
    "        # internal vars\n",
    "        self.iter = 0\n",
    "        self.mem_pointer = 0 # memory pointer\n",
    "        self.max_pointer = 0\n",
    "        self.db = None # large array of states seen\n",
    "        self.dba = {} # actions taken\n",
    "        self.dbr = {} # rewards obtained at all steps\n",
    "        self.dbv = {} # value function at all steps, computed retrospectively\n",
    "        self.ep_start_pointer = 0\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        assert isinstance(observation, np.ndarray) and observation.ndim == 1, 'unsupported observation type for now.'\n",
    "\n",
    "        if self.db is None:\n",
    "            # lazy initialization of memory\n",
    "            self.db = np.zeros((self.mem_size, observation.size))\n",
    "            self.mem_pointer = 0\n",
    "            self.ep_start_pointer = 0\n",
    "\n",
    "        # we have enough data, we want to explore, and we have seen at least one episode already (so values were computed)\n",
    "        if self.iter > self.mem_needed and np.random.rand() > self.epsilon and self.dbv:\n",
    "            # exploit: find the few closest states and pick the action that led to highest rewards\n",
    "            # 1. find k nearest neighbors\n",
    "            ds = np.sum((self.db[:self.max_pointer] - observation)**2, axis=1) # L2 distance\n",
    "            ix = np.argsort(ds) # sorts ascending by distance\n",
    "            ix = ix[:min(len(ix), self.nnfind)] # crop to only some number of nearest neighbors\n",
    "            \n",
    "            # find the action that leads to most success. do a vote among actions\n",
    "            adict = {}\n",
    "            ndict = {}\n",
    "            for i in ix:\n",
    "                vv = self.dbv[i]\n",
    "                aa = self.dba[i]\n",
    "                vnew = adict.get(aa, 0) + vv\n",
    "                adict[aa] = vnew\n",
    "                ndict[aa] = ndict.get(aa, 0) + 1\n",
    "\n",
    "            for a in adict: # normalize by counts\n",
    "                adict[a] = adict[a] / ndict[a]\n",
    "\n",
    "            its = [(y,x) for x,y in adict.iteritems()]\n",
    "            its.sort(reverse=True) # descending\n",
    "            a = its[0][1]\n",
    "\n",
    "        else:\n",
    "            # explore: do something random\n",
    "            a = self.action_space.sample()\n",
    "\n",
    "        # record move to database\n",
    "        if self.mem_pointer < self.mem_size:\n",
    "            self.db[self.mem_pointer] = observation # save the state\n",
    "            self.dba[self.mem_pointer] = a # and the action we took\n",
    "            self.dbr[self.mem_pointer-1] = reward # and the reward we obtained last time step\n",
    "            self.dbv[self.mem_pointer-1] = 0\n",
    "        self.mem_pointer += 1\n",
    "        self.iter += 1\n",
    "\n",
    "        if done: # episode Ended;\n",
    "\n",
    "            # compute the estimate of the value function based on this rollout\n",
    "            v = 0\n",
    "            for t in reversed(xrange(self.ep_start_pointer, self.mem_pointer)):\n",
    "                v = self.gamma * v + self.dbr.get(t,0)\n",
    "                self.dbv[t] = v\n",
    "\n",
    "            self.ep_start_pointer = self.mem_pointer\n",
    "            self.max_pointer = min(max(self.max_pointer, self.mem_pointer), self.mem_size)\n",
    "\n",
    "            # decay exploration probability\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min) # cap at epsilon_min\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-20 17:34:12,922] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 running reward: 0.650000\n",
      "100 running reward: 140.665248\n",
      "200 running reward: 196.045064\n",
      "300 running reward: 198.917342\n",
      "400 running reward: 197.071849\n"
     ]
    }
   ],
   "source": [
    "agent = EpisodicAgent(env.action_space)\n",
    "\n",
    "episode_count = 500\n",
    "max_steps = 200\n",
    "reward = 0\n",
    "done = False\n",
    "sum_reward_running = 0\n",
    "\n",
    "for i in xrange(episode_count):\n",
    "    ob = env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    for j in xrange(max_steps):\n",
    "        action = agent.act(ob, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    sum_reward_running = sum_reward_running * 0.95 + sum_reward * 0.05\n",
    "    if i%100 == 0:\n",
    "        print '%d running reward: %f' % (i, sum_reward_running)\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize performance on new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0\n"
     ]
    }
   ],
   "source": [
    "reward = 0\n",
    "sum_reward = 0\n",
    "done = False\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        action = agent.act(observation, reward, done)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        sum_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(sum_reward)\n",
    "            break\n",
    "    \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Using neural networks\n",
    "\n",
    "Based on [Arthur Juliani's blog](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.722zqqrr2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "H = 10               # Number of hidden neurons\n",
    "batch_size = 50      # How many episodes before updating parameters\n",
    "learning_rate = 1e-2 # Learning rate\n",
    "gamma = 0.99         # Discount factor\n",
    "D = 4                # Input dimensionality (env.observation_space size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define the network architecture. Takes observation as an input, and outputs probability of going right or left\n",
    "observations = tf.placeholder(tf.float32, [None,D], name='input_x')\n",
    "W1 = tf.get_variable('W1', shape=[D,H],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable('W2', shape=[H,1],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1],name='inpuy_y')\n",
    "advantages = tf.placeholder(tf.float32,name='reward_signal')\n",
    "\n",
    "# Loss function: Try to make good actions more likely\n",
    "loglik = tf.log(input_y*(input_y-probability) + (1-input_y)*(input_y+probability))\n",
    "loss = -tf.reduce_mean(loglik*advantages)\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# We apply the gradients once we have collected them from multiple episodes (stability)\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Placeholders to send the final gradients when we update\n",
    "W1Grad = tf.placeholder(tf.float32,name='batch_grad1')\n",
    "W2Grad = tf.placeholder(tf.float32,name='batch_grad2')\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "Weights the rewards the agent receives. Actions which lead to the pole falling have a decreased or negative reward, while actions which keep the pole in the air have a large reward. Actions at the end of the episode are seen as negative because they likely led to failure. Early actions are seen as positive because they were not responsible for the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes 1D array of rewards and returns a discounted one\n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0,r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 17.260000. Total average reward 17.260000.\n",
      "Average reward for episode 16.700000. Total average reward 17.254400.\n",
      "Average reward for episode 19.340000. Total average reward 17.275256.\n",
      "Average reward for episode 19.820000. Total average reward 17.300703.\n",
      "Average reward for episode 19.160000. Total average reward 17.319296.\n",
      "Average reward for episode 18.620000. Total average reward 17.332303.\n",
      "Average reward for episode 20.500000. Total average reward 17.363980.\n",
      "Average reward for episode 19.140000. Total average reward 17.381741.\n",
      "Average reward for episode 19.700000. Total average reward 17.404923.\n",
      "Average reward for episode 17.920000. Total average reward 17.410074.\n",
      "Average reward for episode 20.740000. Total average reward 17.443373.\n",
      "Average reward for episode 20.620000. Total average reward 17.475139.\n",
      "Average reward for episode 18.620000. Total average reward 17.486588.\n",
      "Average reward for episode 23.140000. Total average reward 17.543122.\n",
      "Average reward for episode 26.340000. Total average reward 17.631091.\n",
      "Average reward for episode 23.700000. Total average reward 17.691780.\n",
      "Average reward for episode 23.600000. Total average reward 17.750862.\n",
      "Average reward for episode 23.160000. Total average reward 17.804954.\n",
      "Average reward for episode 21.960000. Total average reward 17.846504.\n",
      "Average reward for episode 21.560000. Total average reward 17.883639.\n",
      "Average reward for episode 31.000000. Total average reward 18.014803.\n",
      "Average reward for episode 24.760000. Total average reward 18.082255.\n",
      "Average reward for episode 26.260000. Total average reward 18.164032.\n",
      "Average reward for episode 27.580000. Total average reward 18.258192.\n",
      "Average reward for episode 28.560000. Total average reward 18.361210.\n",
      "Average reward for episode 28.320000. Total average reward 18.460798.\n",
      "Average reward for episode 27.220000. Total average reward 18.548390.\n",
      "Average reward for episode 30.400000. Total average reward 18.666906.\n",
      "Average reward for episode 31.540000. Total average reward 18.795637.\n",
      "Average reward for episode 28.920000. Total average reward 18.896880.\n",
      "Average reward for episode 29.420000. Total average reward 19.002112.\n",
      "Average reward for episode 29.940000. Total average reward 19.111491.\n",
      "Average reward for episode 31.440000. Total average reward 19.234776.\n",
      "Average reward for episode 30.920000. Total average reward 19.351628.\n",
      "Average reward for episode 34.060000. Total average reward 19.498712.\n",
      "Average reward for episode 34.660000. Total average reward 19.650324.\n",
      "Average reward for episode 31.300000. Total average reward 19.766821.\n",
      "Average reward for episode 33.640000. Total average reward 19.905553.\n",
      "Average reward for episode 38.820000. Total average reward 20.094698.\n",
      "Average reward for episode 36.760000. Total average reward 20.261351.\n",
      "Average reward for episode 38.300000. Total average reward 20.441737.\n",
      "Average reward for episode 28.820000. Total average reward 20.525520.\n",
      "Average reward for episode 30.160000. Total average reward 20.621864.\n",
      "Average reward for episode 31.940000. Total average reward 20.735046.\n",
      "Average reward for episode 34.200000. Total average reward 20.869695.\n",
      "Average reward for episode 27.460000. Total average reward 20.935598.\n",
      "Average reward for episode 30.180000. Total average reward 21.028042.\n",
      "Average reward for episode 29.020000. Total average reward 21.107962.\n",
      "Average reward for episode 33.460000. Total average reward 21.231482.\n",
      "Average reward for episode 35.520000. Total average reward 21.374368.\n",
      "Average reward for episode 38.940000. Total average reward 21.550024.\n",
      "Average reward for episode 35.820000. Total average reward 21.692724.\n",
      "Average reward for episode 32.720000. Total average reward 21.802996.\n",
      "Average reward for episode 47.060000. Total average reward 22.055566.\n",
      "Average reward for episode 36.360000. Total average reward 22.198611.\n",
      "Average reward for episode 37.200000. Total average reward 22.348625.\n",
      "Average reward for episode 38.780000. Total average reward 22.512938.\n",
      "Average reward for episode 37.180000. Total average reward 22.659609.\n",
      "Average reward for episode 36.260000. Total average reward 22.795613.\n",
      "Average reward for episode 44.120000. Total average reward 23.008857.\n",
      "Average reward for episode 39.160000. Total average reward 23.170368.\n",
      "Average reward for episode 46.380000. Total average reward 23.402465.\n",
      "Average reward for episode 43.840000. Total average reward 23.606840.\n",
      "Average reward for episode 43.060000. Total average reward 23.801372.\n",
      "Average reward for episode 45.980000. Total average reward 24.023158.\n",
      "Average reward for episode 41.880000. Total average reward 24.201726.\n",
      "Average reward for episode 44.120000. Total average reward 24.400909.\n",
      "Average reward for episode 49.320000. Total average reward 24.650100.\n",
      "Average reward for episode 45.520000. Total average reward 24.858799.\n",
      "Average reward for episode 42.080000. Total average reward 25.031011.\n",
      "Average reward for episode 45.180000. Total average reward 25.232501.\n",
      "Average reward for episode 48.880000. Total average reward 25.468976.\n",
      "Average reward for episode 45.580000. Total average reward 25.670086.\n",
      "Average reward for episode 54.240000. Total average reward 25.955785.\n",
      "Average reward for episode 45.140000. Total average reward 26.147627.\n",
      "Average reward for episode 59.660000. Total average reward 26.482751.\n",
      "Average reward for episode 55.200000. Total average reward 26.769924.\n",
      "Average reward for episode 54.740000. Total average reward 27.049624.\n",
      "Average reward for episode 56.320000. Total average reward 27.342328.\n",
      "Average reward for episode 62.760000. Total average reward 27.696505.\n",
      "Average reward for episode 63.000000. Total average reward 28.049540.\n",
      "Average reward for episode 58.340000. Total average reward 28.352444.\n",
      "Average reward for episode 67.020000. Total average reward 28.739120.\n",
      "Average reward for episode 62.040000. Total average reward 29.072129.\n",
      "Average reward for episode 68.980000. Total average reward 29.471207.\n",
      "Average reward for episode 76.380000. Total average reward 29.940295.\n",
      "Average reward for episode 74.000000. Total average reward 30.380892.\n",
      "Average reward for episode 75.200000. Total average reward 30.829083.\n",
      "Average reward for episode 76.860000. Total average reward 31.289393.\n",
      "Average reward for episode 90.480000. Total average reward 31.881299.\n",
      "Average reward for episode 93.020000. Total average reward 32.492686.\n",
      "Average reward for episode 94.480000. Total average reward 33.112559.\n",
      "Average reward for episode 97.800000. Total average reward 33.759433.\n",
      "Average reward for episode 86.040000. Total average reward 34.282239.\n",
      "Average reward for episode 95.920000. Total average reward 34.898617.\n",
      "Average reward for episode 88.480000. Total average reward 35.434430.\n",
      "Average reward for episode 98.540000. Total average reward 36.065486.\n",
      "Average reward for episode 111.160000. Total average reward 36.816431.\n",
      "Average reward for episode 113.060000. Total average reward 37.578867.\n",
      "Average reward for episode 102.060000. Total average reward 38.223678.\n",
      "Average reward for episode 122.320000. Total average reward 39.064641.\n",
      "Average reward for episode 89.900000. Total average reward 39.572995.\n",
      "Average reward for episode 110.020000. Total average reward 40.277465.\n",
      "Average reward for episode 130.900000. Total average reward 41.183690.\n",
      "Average reward for episode 112.800000. Total average reward 41.899854.\n",
      "Average reward for episode 132.100000. Total average reward 42.801855.\n",
      "Average reward for episode 133.900000. Total average reward 43.712836.\n",
      "Average reward for episode 145.940000. Total average reward 44.735108.\n",
      "Average reward for episode 144.420000. Total average reward 45.731957.\n",
      "Average reward for episode 141.300000. Total average reward 46.687637.\n",
      "Average reward for episode 134.680000. Total average reward 47.567561.\n",
      "Average reward for episode 146.640000. Total average reward 48.558285.\n",
      "Average reward for episode 148.680000. Total average reward 49.559503.\n",
      "Average reward for episode 162.600000. Total average reward 50.689908.\n",
      "Average reward for episode 156.880000. Total average reward 51.751808.\n",
      "Average reward for episode 128.260000. Total average reward 52.516890.\n",
      "Average reward for episode 154.980000. Total average reward 53.541521.\n",
      "Average reward for episode 174.720000. Total average reward 54.753306.\n",
      "Average reward for episode 174.240000. Total average reward 55.948173.\n",
      "Average reward for episode 182.440000. Total average reward 57.213091.\n",
      "Average reward for episode 165.420000. Total average reward 58.295161.\n",
      "Average reward for episode 165.780000. Total average reward 59.370009.\n",
      "Average reward for episode 178.380000. Total average reward 60.560109.\n",
      "Average reward for episode 195.680000. Total average reward 61.911308.\n",
      "Average reward for episode 177.000000. Total average reward 63.062195.\n",
      "Average reward for episode 184.400000. Total average reward 64.275573.\n",
      "Average reward for episode 185.280000. Total average reward 65.485617.\n",
      "Average reward for episode 225.780000. Total average reward 67.088561.\n",
      "('Task solved in ', 6400, ' episodes!')\n",
      "(6400, 'Episodes completed')\n"
     ]
    }
   ],
   "source": [
    "xs, drs, ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()     # Resets and obtains initial observation\n",
    "    \n",
    "    # Reset gradient placeholder\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while episode_number <= total_episodes:\n",
    "        # Only render once we achieve a good performance\n",
    "        '''if reward_sum/batch_size > 100 or rendering == True:\n",
    "            env.render()\n",
    "            rendering = True\n",
    "        '''\n",
    "        \n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the network and get an action\n",
    "        tfprob = sess.run(probability,feed_dict={observations:x})\n",
    "        action = 1 if np.random.uniform()<tfprob else 0\n",
    "        \n",
    "        xs.append(x)  # Observation\n",
    "        y = 1 if action == 0 else 0  # a \"fake\" label\n",
    "        ys.append(y)\n",
    "        \n",
    "        # Execute action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        drs.append(reward) # Store reward\n",
    "        \n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            # Stack together all inputs, action gradients and rewards on this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            xs, drs, ys = [],[],[]   # Reset array memory\n",
    "            \n",
    "            # Compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # Size the rewards to be unit normal (helps control the gradients)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode and save it in gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations:epx, input_y:epy, advantages:discounted_epr})\n",
    "            for ix, grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy with the gradients\n",
    "            if episode_number%batch_size == 0:\n",
    "                sess.run(updateGrads,feed_dict={W1Grad:gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                    \n",
    "                # Give a performance summary \n",
    "                running_reward = reward_sum if running_reward is None else running_reward *0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode %f. Total average reward %f.' % (reward_sum/batch_size,running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 200:\n",
    "                    print('Task solved in ', episode_number, ' episodes!')\n",
    "                    break\n",
    "                \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "                \n",
    "print(episode_number, 'Episodes completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
