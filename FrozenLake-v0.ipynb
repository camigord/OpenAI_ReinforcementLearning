{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0\n",
    "\n",
    "Solving [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0) using a simple implementation of Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the environment from gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 13:14:45,935] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "        self.gamma = 0.9    # Discount factor\n",
    "        self.l_r = 0.7      # Learning rate\n",
    "        # self.epsilon = 1.0  # Probability of choosing a random action\n",
    "        # self.epsilon_decay = 0.98 # Decay of epsilon per episode\n",
    "        # self.epsilon_min = 0\n",
    "        \n",
    "        self.Q_Values = np.zeros((self.observation_space.n, self.action_space.n))\n",
    "        \n",
    "    def act(self, state, episode):\n",
    "        # Choose an action greedily. We add random but decaying noise to the selection in order to explore actions.\n",
    "        a = np.argmax(self.Q_Values[state,:] + np.random.randn(1,self.action_space.n)*(1./(episode+1)))\n",
    "        '''if np.random.rand() > self.epsilon:\n",
    "            print('exploit')\n",
    "            a = np.argmax(self.Q_Values[state,:])\n",
    "        else:\n",
    "            print('explore')\n",
    "            # explore: do something random\n",
    "            a = self.action_space.sample()\n",
    "        '''\n",
    "        return a\n",
    "\n",
    "    def update(self, previous_state, new_state, reward, action):\n",
    "        self.Q_Values[previous_state,action] += self.l_r*(reward + self.gamma*np.max(self.Q_Values[new_state,:]) - self.Q_Values[previous_state,action])        \n",
    "    \n",
    "    '''def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min) # cap at epsilon_min\n",
    "    '''\n",
    "            \n",
    "    def get_Qvalues(self):\n",
    "        return self.Q_Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n",
      "1819\n",
      "0.78\n",
      "Final Q-Table Values\n",
      "[[  3.95482736e-02   1.75628494e-03   2.89784263e-03   1.64754761e-03]\n",
      " [  1.06723964e-01   1.52008930e-03   1.21345509e-03   1.24235503e-03]\n",
      " [  1.51974005e-03   3.39117786e-03   1.22331653e-03   3.92982668e-02]\n",
      " [  2.41155861e-03   3.50534800e-04   3.03159121e-04   1.87026113e-02]\n",
      " [  9.53957683e-02   8.52459166e-04   1.67970344e-03   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  8.14636024e-02   8.08431649e-05   1.14538759e-05   4.28579764e-07]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.54467818e-03   1.02972432e-02   2.23626996e-04   2.08916952e-01]\n",
      " [  0.00000000e+00   4.61353980e-01   0.00000000e+00   8.26456566e-04]\n",
      " [  8.07378558e-01   5.76753312e-04   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   3.97818895e-04   7.79065913e-01   8.84028989e-04]\n",
      " [  0.00000000e+00   9.98448418e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "max_steps_per_episode = 100\n",
    "hist_reward = []     # List to store the total reward collected per episode\n",
    "\n",
    "# Create learning agent\n",
    "agent = Agent(env.action_space, env.observation_space)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    s = env.reset()  # Resets the environment and returns first observation (the state of the environment)\n",
    "    cum_reward = 0   # Cumulative reward per episode\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose and action\n",
    "        a = agent.act(s,episode)\n",
    "        \n",
    "        # Execute action and get new state and reward\n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "        # Update agent\n",
    "        agent.update(s, new_state, reward, a)\n",
    "        cum_reward += reward\n",
    "        s = new_state\n",
    "        if done == True:\n",
    "            # Decay exploration probability\n",
    "            #agent.decay_epsilon()\n",
    "            break\n",
    "    \n",
    "    hist_reward.append(cum_reward)\n",
    "    if np.mean(hist_reward[-100:]) >= 0.78:\n",
    "        print('Finish')\n",
    "        print(episode)\n",
    "        break\n",
    "    \n",
    "print(np.mean(hist_reward[-100:]))\n",
    "\n",
    "print('Final Q-Table Values')\n",
    "print(agent.get_Qvalues())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Playing with the rewards\n",
    "\n",
    "In the 'FrozenLake-v0' environment the agent receives a reward of 0 whenever he executes an action which does not lead to the goal. However, there is no difference between moving around the 'Lake' and falling into one of the 'holes'. Here, I will modify the reward function in order to get a small punishment (-0.05) whenever we fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n",
      "1972\n",
      "0.7825\n",
      "Final Q-Table Values\n",
      "[[  1.03269174e-01   2.73869097e-03   2.46885457e-03   2.61483797e-03]\n",
      " [ -4.65323464e-02  -3.60587811e-02  -4.86599509e-02   8.80415070e-02]\n",
      " [ -2.30908807e-02  -2.76049032e-02  -3.55143667e-02   5.38075824e-02]\n",
      " [ -3.52835000e-02  -4.86500000e-02  -4.55000000e-02   2.91813427e-02]\n",
      " [  1.85126992e-01  -4.95950000e-02  -4.55023150e-02  -3.93662383e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -4.93644199e-02  -4.95979737e-02   1.02513117e-01  -4.99897544e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -3.53685500e-02  -4.55000000e-02  -4.55000000e-02   3.87247352e-01]\n",
      " [ -3.81500000e-02   3.98615049e-01  -3.50000000e-02  -4.55000000e-02]\n",
      " [  7.49873861e-01  -4.57835000e-02  -4.16599005e-02  -4.16134754e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -3.50000000e-02   0.00000000e+00   7.88909160e-01  -2.58453872e-03]\n",
      " [ -8.79007812e-05   9.97056057e-01   0.00000000e+00  -2.13152595e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "max_steps_per_episode = 100\n",
    "hist_reward = []     # List to store the total reward collected per episode\n",
    "\n",
    "# Create learning agent\n",
    "agent = Agent(env.action_space, env.observation_space)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    s = env.reset()  # Resets the environment and returns first observation (the state of the environment)\n",
    "    cum_reward = 0   # Cumulative reward per episode\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose and action\n",
    "        a = agent.act(s,episode)\n",
    "        \n",
    "        # Execute action and get new state and reward\n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "        \n",
    "        # Modifying the reward\n",
    "        if reward != 1.0 and done == True:\n",
    "            reward = -0.05\n",
    "\n",
    "        # Update agent\n",
    "        agent.update(s, new_state, reward, a)\n",
    "        cum_reward += reward\n",
    "        s = new_state\n",
    "        if done == True:\n",
    "            # Decay exploration probability\n",
    "            #agent.decay_epsilon()\n",
    "            break\n",
    "    \n",
    "    hist_reward.append(cum_reward)\n",
    "    if np.mean(hist_reward[-100:]) >= 0.78:\n",
    "        print('Finish')\n",
    "        print(episode)\n",
    "        break\n",
    "    \n",
    "print(np.mean(hist_reward[-100:]))\n",
    "\n",
    "print('Final Q-Table Values')\n",
    "print(agent.get_Qvalues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
