{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Async. one-step Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import shutil\n",
    "import threading\n",
    "import multiprocessing\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import Gym_Auxiliar as Aux_Gym\n",
    "\n",
    "from inspect import getsourcefile\n",
    "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
    "import_path = os.path.abspath(os.path.join(current_path, \"..\"))\n",
    "\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "import commonOps as cops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    'env_name': 'Breakout-v0',\n",
    "    't_max': 5,          # Update frequency\n",
    "    'max_global_episodes': 1000000,\n",
    "    \n",
    "    'update_target': 2000,\n",
    "    \n",
    "    'eval_steps': 300,             # Evaluate the policy every N seconds\n",
    "    'reset': False,                # If set, delete the existing model directory and start training from scratch.\n",
    "    'num_threads': 4,              # Number of threads to run.\n",
    "    \n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    \n",
    "    # Gym environment\n",
    "    'pool_frame_size': 1,\n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    'action_repeat': 1,           # How many times should the same action be taken\n",
    "    \n",
    "    'batch_size': 32,             # Number of training cases ove which SGD update is computed\n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "}\n",
    "\n",
    "# Set the number of workers\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "if conf_parameters['num_threads']:\n",
    "    num_workers = conf_parameters['num_threads']\n",
    "\n",
    "# Optionally empty model directory\n",
    "if conf_parameters['reset']:\n",
    "    shutil.rmtree(conf_parameters['checkpoint_dir'], ignore_errors=True)\n",
    "    shutil.rmtree(conf_parameters['log_dir'], ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class defining global shared networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet_Estimator():\n",
    "    '''Q-value estimator neural network\n",
    "       This architecture will be used both for the Q-network and the Target network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,conf,num_actions,net_type=0):\n",
    "        if net_type == 0:\n",
    "            self.scope = 'Global_Training'\n",
    "            self.collection = 'Normal' \n",
    "        else:\n",
    "            self.scope = 'Global_Target'\n",
    "            self.collection = 'Target'\n",
    "            \n",
    "        self.num_actions = num_actions\n",
    "        self.screen_width = conf['screen_width']\n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.history_length = conf['history_length']\n",
    "        self.batch_size = conf['batch_size']\n",
    "        self.learning_rate = conf['learning_rate']\n",
    "        self.gamma = conf['gamma']\n",
    "      \n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "                \n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        Building the network architecture\n",
    "        '''\n",
    "        self.state_ph = tf.placeholder(tf.float32,[None, self.screen_height, self.screen_width, self.history_length],name='X')\n",
    "        \n",
    "        self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.state_ph,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID', scope=\"conv1\")\n",
    "        self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID', scope=\"conv2\")\n",
    "        hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu, scope=\"FC\")\n",
    "\n",
    "        self.Q_output = slim.fully_connected(hidden,self.num_actions,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                biases_initializer=None, scope='Output')\n",
    "        \n",
    "        with tf.name_scope(\"Best_Action\"):\n",
    "            self.best_action = tf.argmax(self.Q_output,dimension=1)\n",
    "        \n",
    "        if self.scope != 'Global_Target':  \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                with tf.name_scope(\"Inputs\"):\n",
    "                    # The TD target value\n",
    "                    self.QT_ph = tf.placeholder(tf.float32,[None],name='QT_ph')\n",
    "                    # Integer id of selected action\n",
    "                    self.action_ph = tf.placeholder(tf.int32,[None],name='action_ph')\n",
    "\n",
    "                    self.reward_ph = tf.placeholder(tf.float32, [None], name=\"reward_ph\")\n",
    "                    self.terminal_ph = tf.placeholder(tf.float32, [None], name=\"terminal_ph\")\n",
    "                \n",
    "                with tf.name_scope(\"Acted_Q\"):\n",
    "                    # One hot of the action which was taken\n",
    "                    action_one_hot = tf.one_hot(self.action_ph,self.num_actions, 1., 0., name='action_one_hot')\n",
    "                    # Get the prediction of the chosen actions only\n",
    "                    acted_Q = tf.reduce_sum(self.Q_output * action_one_hot, reduction_indices=1, name='DQN_acted')\n",
    "\n",
    "                with tf.name_scope(\"Target_Q\"):\n",
    "                    Y = self.reward_ph + self.gamma * self.QT_ph * (1 - self.terminal_ph)\n",
    "                    Y = tf.stop_gradient(Y)\n",
    "\n",
    "                loss_batch = cops.clipped_l2(Y, acted_Q)\n",
    "                loss = tf.reduce_sum(loss_batch, name=\"loss\")\n",
    "        \n",
    "                self.train_op, grads = cops.graves_rmsprop_optimizer(loss, self.learning_rate, 0.95, 0.01, 1)\n",
    "        \n",
    "            with tf.name_scope(\"Summaries\"):\n",
    "                # Summaries for Tensorboard\n",
    "                self.summaries = tf.merge_summary([\n",
    "                        tf.scalar_summary('losses/loss', loss),\n",
    "                        tf.scalar_summary(\"losses/loss_max\", tf.reduce_max(loss_batch)),\n",
    "                        tf.scalar_summary('Q/avg_q',tf.reduce_mean(self.Q_output)),\n",
    "                        tf.histogram_summary('Q/q_values_hist',self.Q_output),\n",
    "                        tf.scalar_summary('Q/max_q_value',tf.reduce_max(self.Q_output)),\n",
    "                        tf.scalar_summary('Others/reward_max', tf.reduce_max(self.reward_ph))\n",
    "                        ])  \n",
    "                \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predicts action values.\n",
    "        '''\n",
    "        return sess.run(self.Q_output,{self.state_ph:state})\n",
    "    \n",
    "    def determine_action(self,sess,state,epsilon):\n",
    "        '''\n",
    "        Predicts action based on q values for current state and exploration strategy\n",
    "        '''\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: random action\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:            \n",
    "            action = sess.run(self.best_action,{self.state_ph:[state]})[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self,sess,state,action,reward,target,terminal):\n",
    "        '''\n",
    "        Updates the estimator towards the given targets\n",
    "        '''\n",
    "        feed_dict = {self.state_ph:state, self.QT_ph:target, \n",
    "                     self.action_ph:action, self.reward_ph:reward,\n",
    "                     self.terminal_ph:terminal}\n",
    "        _, summaries = sess.run([self.train_op, self.summaries],feed_dict)\n",
    "\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Monitoring_Worker():\n",
    "    '''\n",
    "    Monitors the global step, updates target network and test progress.\n",
    "    '''\n",
    "    def __init__(self, name, conf, Q_net, Target_net, global_episodes, summary_writer=None):\n",
    "        self.name = name\n",
    "        self.max_global_episodes = conf['max_global_episodes']\n",
    "        self.update_target_freq = conf['update_target']\n",
    "        self.global_Q_net = Q_net\n",
    "        self.global_Target_net = Target_net\n",
    "        self.global_episodes = global_episodes\n",
    "        self.summary_writer = summary_writer\n",
    "        self.eval_env = Aux_Gym.GymEnvironment(conf_parameters['env_name'],conf_parameters)\n",
    "        \n",
    "        with tf.name_scope(\"Copy_Parameters\"):\n",
    "            self.update_target_ops = self.update_target_graph(self.global_Q_net.scope,self.global_Target_net.scope) \n",
    "        \n",
    "    def update_target_graph(self,from_scope,to_scope):\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(from_vars,to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "        \n",
    "    def run(self, sess, coord):\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    episode_count = sess.run(self.global_episodes)\n",
    "                \n",
    "                    if episode_count % self.update_target_freq == 0:\n",
    "                        sess.run(self.update_target_ops)\n",
    "                        print('updated')\n",
    "                        \n",
    "                    if episode_count >= self.max_global_episodes:\n",
    "                        print('finish')\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "\n",
    "            except tf.errors.CancelledError:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_Network = ConvNet_Estimator(conf_parameters,6,net_type=0)\n",
    "Target_Network = ConvNet_Estimator(conf_parameters,6,net_type=1)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    #summary_writer = tf.train.SummaryWriter(conf_parameters['log_dir'], flush_secs=30)\n",
    "    \n",
    "    Eval_worker = Monitoring_Worker(\n",
    "            name='Worker_0',\n",
    "            conf=conf_parameters,\n",
    "            Q_net=Q_Network,\n",
    "            Target_net=Target_Network,\n",
    "            global_episodes=global_episodes,\n",
    "            summary_writer = [])\n",
    "    \n",
    "    Eval_worker2 = Monitoring_Worker(\n",
    "            name='Worker_1',\n",
    "            conf=conf_parameters,\n",
    "            Q_net=Q_Network,\n",
    "            Target_net=Target_Network,\n",
    "            global_episodes=global_episodes,\n",
    "            summary_writer = [])\n",
    "        \n",
    "with tf.Session() as sess: \n",
    "    summary_writer = tf.train.SummaryWriter(conf_parameters['log_dir'],sess.graph, flush_secs=30)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    # Start a thread\n",
    "    worker_threads = []\n",
    "    eval_thread = threading.Thread(target=lambda: Eval_worker.run(sess, coord))\n",
    "    eval_thread.start()\n",
    "    worker_threads.append(eval_thread)\n",
    "    \n",
    "    eval_thread2 = threading.Thread(target=lambda: Eval_worker2.run(sess, coord))\n",
    "    eval_thread2.start()\n",
    "    worker_threads.append(eval_thread2)\n",
    "\n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the workers\n",
    "\n",
    "Each worker will run in a different thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    '''\n",
    "    An A3C worker thread. Runs episodes locally and updates global shared value and policy nets.\n",
    "    '''\n",
    "    def __init__(self, name, conf, Q_net, Target_net, global_counter, summary_writer=None):\n",
    "        self.name = name\n",
    "        self.gamma = conf['gamma']\n",
    "        self.max_global_episodes = conf['max_global_episodes']\n",
    "        # self.global_step = tf.contrib.framework.get_global_step()\n",
    "        self.global_Q_net = Q_net\n",
    "        self.global_Target_net = Target_net\n",
    "        self.global_counter = global_counter\n",
    "        self.local_counter = itertools.count()\n",
    "        self.summary_writer = summary_writer\n",
    "        self.env = Aux_Gym.GymEnvironment(conf_parameters['env_name'],conf_parameters)\n",
    "        \n",
    "    def run(self, sess, coord, t_max):\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            # Initial state\n",
    "            self.state = self.env.new_game()\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    # Copy Parameters from the global networks\n",
    "                    sess.run(self.copy_params_op)\n",
    "\n",
    "                    # Collect some experience\n",
    "                    transitions, local_t, global_t = self.run_n_steps(t_max, sess)\n",
    "\n",
    "                    if self.max_global_steps is not None and global_t >= self.max_global_steps:\n",
    "                        tf.logging.info(\"Reached global step {}. Stopping.\".format(global_t))\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "\n",
    "                    # Update the global networks\n",
    "                    self.update(transitions, sess)\n",
    "\n",
    "            except tf.errors.CancelledError:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the threads and global networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(conf_parameters['env_name'])\n",
    "action_size = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Keeps track of the number of updates we've performed\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # Global shared networks\n",
    "    with tf.variable_scope(\"global\"):\n",
    "        Q_Network = ConvNet_Estimator(conf_parameters,action_size,net_type=0)\n",
    "        Target_Network = ConvNet_Estimator(conf_parameters,action_size,net_type=1)\n",
    "\n",
    "    # Global step iterator\n",
    "    global_counter = itertools.count()\n",
    "\n",
    "    # Create worker graphs\n",
    "    workers = []\n",
    "    for worker_id in range(num_workers):\n",
    "        # We only write summaries in one of the workers because they're pretty much identical.\n",
    "        worker_summary_writer = None\n",
    "        if worker_id == 0:\n",
    "            worker_summary_writer = summary_writer\n",
    "\n",
    "        # TODO: Define Worker class\n",
    "        worker = Worker(\n",
    "            name=\"worker_{}\".format(worker_id),\n",
    "            env=gym.make(conf_parameters['env']),\n",
    "            q_estimator=q_estimator,\n",
    "            target_estimator=target_estimator,\n",
    "            global_counter=global_counter,\n",
    "            discount_factor = 0.99,\n",
    "            summary_writer=worker_summary_writer,\n",
    "            max_global_steps=conf_parameters['max_steps'])\n",
    "        workers.append(worker)\n",
    "\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.0, max_to_keep=10)\n",
    "\n",
    "    # Used to occasionally write episode rewards to Tensorboard\n",
    "    # TODO: Define monitor\n",
    "    pe = PolicyMonitor(\n",
    "        env= gym.make(conf_parameters['env']),\n",
    "        q_estimator=q_estimator,\n",
    "        summary_writer=summary_writer,\n",
    "        saver=saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # Load a previous checkpoint if it exists\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(conf_parameters['checkpoint_dir'])\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint: {}\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    # Start worker threads\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_fn = lambda: worker.run(sess, coord, conf_parameters['t_max'])\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "\n",
    "    # Start a thread for policy eval task\n",
    "    monitor_thread = threading.Thread(target=lambda: pe.continuous_eval(conf_parameters['eval_steps'], sess, coord))\n",
    "    monitor_thread.start()\n",
    "\n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
