{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Async. one-step Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilog/Anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import itertools\n",
    "import shutil\n",
    "import threading\n",
    "import multiprocessing\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import time\n",
    "\n",
    "from inspect import getsourcefile\n",
    "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
    "import_path = os.path.abspath(os.path.join(current_path, \"..\"))\n",
    "\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "    \n",
    "sys.path.remove('/home/camilog/tensorflow/_python_build')\n",
    "\n",
    "import commonOps as cops\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'num_episodes':50000,\n",
    "    \n",
    "    'eval_freq': 500,\n",
    "    'eval_episodes': 5,\n",
    "    \n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    \n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "         \n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'ep_min': 0.1,                 # Final exploration\n",
    "    'ep_start': 1.0,               # Initial exploration\n",
    " \n",
    "    'target_q_update_step': 50,   # Target network update frequency\n",
    "    'log_online_summary_rate': 50,   \n",
    "    'save_rate': 1000,\n",
    "    'async_update': 10,\n",
    "    \n",
    "    # Clip rewards\n",
    "    'min_reward': -1.0,\n",
    "    'max_reward': 1.0,\n",
    "\n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    \n",
    "    'env_name': 'Breakout-v0',\n",
    "    'num_threads': 10\n",
    "}\n",
    "\n",
    "# Set the number of workers\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "if conf_parameters['num_threads']:\n",
    "    num_workers = conf_parameters['num_threads']\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class defining global shared networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet_Estimator():\n",
    "    '''Q-value estimator neural network\n",
    "       This architecture will be used both for the Q-network and the Target network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,conf,num_actions,net_type=0):\n",
    "        if net_type == 0:\n",
    "            self.scope = 'Global_Training'\n",
    "            self.collection = 'Normal' \n",
    "        else:\n",
    "            self.scope = 'Global_Target'\n",
    "            self.collection = 'Target'\n",
    "            \n",
    "        self.num_actions = num_actions\n",
    "        self.screen_width = conf['screen_width']\n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.history_length = conf['history_length']\n",
    "        self.learning_rate = conf['learning_rate']\n",
    "        self.gamma = conf['gamma']\n",
    "      \n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "                \n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        Building the network architecture\n",
    "        '''\n",
    "        self.state_ph = tf.placeholder(tf.float32,[None, self.screen_height, self.screen_width, self.history_length],name='X')\n",
    "        self.norm_input = tf.div(self.state_ph,256., name='normalized_input')\n",
    "        \n",
    "        self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.norm_input,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID', scope=\"conv1\")\n",
    "        self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID', scope=\"conv2\")\n",
    "        hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu, scope=\"FC\")\n",
    "\n",
    "        self.Q_output = slim.fully_connected(hidden,self.num_actions,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                biases_initializer=None, scope='Output')\n",
    "        \n",
    "        with tf.name_scope(\"Best_Action\"):\n",
    "            self.best_action = tf.argmax(self.Q_output,dimension=1)\n",
    "        \n",
    "        if self.scope != 'Global_Target':  \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                with tf.name_scope(\"Inputs\"):\n",
    "                    # The TD target value\n",
    "                    self.QT_ph = tf.placeholder(tf.float32,[None],name='QT_ph')\n",
    "                    # Integer id of selected action\n",
    "                    self.action_ph = tf.placeholder(tf.int32,[None],name='action_ph')\n",
    "\n",
    "                    self.reward_ph = tf.placeholder(tf.float32, [None], name=\"reward_ph\")\n",
    "                    self.terminal_ph = tf.placeholder(tf.float32, [None], name=\"terminal_ph\")\n",
    "            \n",
    "                with tf.name_scope(\"Acted_Q\"):\n",
    "                    # One hot of the action which was taken\n",
    "                    action_one_hot = tf.one_hot(self.action_ph,self.num_actions, 1., 0., name='action_one_hot')\n",
    "                    # Get the prediction of the chosen actions only\n",
    "                    acted_Q = tf.reduce_sum(self.Q_output * action_one_hot, reduction_indices=1, name='DQN_acted')\n",
    "\n",
    "                with tf.name_scope(\"Target_Q\"):\n",
    "                    Y = self.reward_ph + self.gamma * self.QT_ph * (1 - self.terminal_ph)\n",
    "                    Y = tf.stop_gradient(Y)\n",
    "\n",
    "                loss_batch = cops.clipped_l2(Y, acted_Q)\n",
    "                loss = tf.reduce_sum(loss_batch, name=\"loss\")\n",
    "        \n",
    "                self.train_op, grads = cops.graves_rmsprop_optimizer(loss, self.learning_rate, 0.95, 0.01, 1)\n",
    "        \n",
    "            with tf.name_scope(\"Summaries\"):\n",
    "                # Summaries for Tensorboard\n",
    "                self.summaries = tf.merge_summary([\n",
    "                        tf.scalar_summary('losses/loss', loss),\n",
    "                        tf.scalar_summary(\"losses/loss_max\", tf.reduce_max(loss_batch)),\n",
    "                        tf.scalar_summary('Q/avg_q',tf.reduce_mean(self.Q_output)),\n",
    "                        tf.histogram_summary('Q/q_values_hist',self.Q_output),\n",
    "                        tf.scalar_summary('Q/max_q_value',tf.reduce_max(self.Q_output)),\n",
    "                        tf.scalar_summary('Others/reward_max', tf.reduce_max(self.reward_ph))\n",
    "                        ])  \n",
    "                \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predicts action values.\n",
    "        '''\n",
    "        return sess.run(self.Q_output,{self.state_ph:state})\n",
    "    \n",
    "    def determine_action(self,sess,state,epsilon):\n",
    "        '''\n",
    "        Predicts action based on q values for current state and exploration strategy\n",
    "        '''\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: random action\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:            \n",
    "            action = sess.run(self.best_action,{self.state_ph:state})[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self,sess,state,action,reward,target,terminal):\n",
    "        '''\n",
    "        Updates the estimator towards the given targets\n",
    "        '''\n",
    "        feed_dict = {self.state_ph:state, self.QT_ph:target, \n",
    "                     self.action_ph:action, self.reward_ph:reward,\n",
    "                     self.terminal_ph:terminal}\n",
    "        _, summaries = sess.run([self.train_op, self.summaries],feed_dict)\n",
    "\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training workers\n",
    "\n",
    "Each worker will run in a different training thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,name, conf, Q_net, Target_net, global_episodes, epsilon, summary_writer=None):\n",
    "        self.name = name \n",
    "        self.config = conf\n",
    "        self.global_Q_net = Q_net\n",
    "        self.global_Target_net = Target_net\n",
    "        self.global_episodes = global_episodes\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        self.env = gym.make(conf['env_name'])      \n",
    "        self.game_state = np.zeros((1, conf['screen_width'], conf['screen_height'], conf['history_length']), dtype=np.uint8)\n",
    "        self.reset_game()\n",
    "        \n",
    "        self.epsilon = epsilon                \n",
    "        \n",
    "        if self.name == 'Worker_0':\n",
    "            self.saver = tf.train.Saver(max_to_keep=5)\n",
    "            \n",
    "            with tf.name_scope(\"Increment_Episode\"):\n",
    "                self.increment = self.global_episodes.assign_add(1)\n",
    "                \n",
    "            with tf.name_scope(\"Copy_Parameters\"):\n",
    "                self.update_target_ops = self.update_target_graph(self.global_Q_net.scope,self.global_Target_net.scope) \n",
    "        \n",
    "    def train(self,sess,episode_buffer):\n",
    "        state, action , reward, end_state, terminal = map(np.array, zip(*episode_buffer))\n",
    "        \n",
    "        # Get the Q-value of the next state\n",
    "        q_t_plus_1 = self.global_Target_net.predict(sess,end_state)\n",
    "        # Get max Q_t+1\n",
    "        target_q = np.max(q_t_plus_1, axis=1)\n",
    "        \n",
    "        summary = self.global_Q_net.update(sess,state,action,reward,target_q,terminal)\n",
    "\n",
    "        return summary\n",
    "    \n",
    "    def update_target_graph(self,from_scope,to_scope):\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(from_vars,to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "    \n",
    "    def test_play(self,sess,test_epsilon=0.01):\n",
    "        ep_rewards = []\n",
    "                \n",
    "        for episode in range(self.config['eval_episodes']):\n",
    "            # New game\n",
    "            screen, terminal, score = self.env.reset(), False, 0\n",
    "\n",
    "            while not terminal:\n",
    "                action = self.step(screen,test_epsilon)\n",
    "                screen, r, terminal, _ = self.env.step(action)\n",
    "                ep_rewards.append(r)\n",
    "                # Record every reward\n",
    "                score += r\n",
    "\n",
    "            ep_rewards.append(score)\n",
    "            self.reset_game()\n",
    "        \n",
    "        return ep_rewards \n",
    "    \n",
    "    def save_model(self,sess,step=None):\n",
    "        print('Saving model at step %d' % step)\n",
    "        self.saver.save(sess,os.path.join(self.config['checkpoint_dir'],'model'),global_step=step) \n",
    "    \n",
    "    def reset_game(self):\n",
    "        self.game_state.fill(0)\n",
    "\n",
    "    def observe(self, x):\n",
    "        x_ = cv2.resize(x, (self.config['screen_width'], self.config['screen_height']))\n",
    "        x_ = cv2.cvtColor(x_, cv2.COLOR_RGB2GRAY)\n",
    "        self.game_state = np.roll(self.game_state, -1, axis=3)\n",
    "        self.game_state[0, :, :, -1] = x_\n",
    "            \n",
    "    def step(self, screen, eps):\n",
    "        self.observe(screen)\n",
    "        self.game_action = self.global_Q_net.determine_action(sess,self.game_state, eps)\n",
    "        return self.game_action\n",
    "        \n",
    "    def run(self,sess,coord):\n",
    "        print \"Starting \" + str(self.name)\n",
    "        with sess.as_default(), sess.graph.as_default(): \n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    episode_count = sess.run(self.global_episodes)\n",
    "                    \n",
    "                    # UPDATE TARGET NETWORK\n",
    "                    if self.name == 'Worker_0':\n",
    "                        if episode_count % self.config['target_q_update_step'] == 0:\n",
    "                            print(self.name + ': Updating target network')\n",
    "                            sess.run(self.update_target_ops)\n",
    "                           \n",
    "                    # TRAIN\n",
    "                    episode_buffer = []\n",
    "                    num_steps = 0\n",
    "                    ep_rewards = []\n",
    "                                     \n",
    "                    screen, r, terminal = self.env.reset(), 0, False\n",
    "                    \n",
    "                    while not terminal:\n",
    "                        prev_state = self.game_state.reshape(self.game_state.shape[1:]) \n",
    "                        action = self.step(screen,self.epsilon)\n",
    "                        screen, r, terminal, _ = self.env.step(action)\n",
    "                        num_steps += 1\n",
    "                        ep_rewards.append(r)\n",
    "                        r = max(self.config['min_reward'], min(self.config['max_reward'], r)) \n",
    "                        \n",
    "                        new_state = self.game_state.reshape(self.game_state.shape[1:]) \n",
    "                        episode_buffer.append([prev_state,action,r,new_state,terminal])\n",
    "                        \n",
    "                        if len(episode_buffer) == self.config['async_update'] and not terminal:\n",
    "                            _ = self.train(sess,episode_buffer)\n",
    "                            episode_buffer = []\n",
    "                    \n",
    "                    self.reset_game()\n",
    "                            \n",
    "                    # Update the network using the experience buffer at the end of the episode.\n",
    "                    if len(episode_buffer) != 0:\n",
    "                        _ = self.train(sess,episode_buffer)  \n",
    "\n",
    "                    # STOP THREAD\n",
    "                    if episode_count >= self.config['num_episodes']:\n",
    "                        print \"Finishing \" + str(self.name)\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "                    \n",
    "                    \n",
    "                    if self.name == 'Worker_0':\n",
    "                        if episode_count % self.config['log_online_summary_rate'] == 0:\n",
    "                            performance_summary = tf.Summary(\n",
    "                                value=[\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"Online/average\",\n",
    "                                        simple_value=sum(ep_rewards)/float(len(ep_rewards))),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"Online/max\",\n",
    "                                        simple_value=max(ep_rewards)),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"Online/min\",\n",
    "                                        simple_value=min(ep_rewards)),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"Online/num_steps\",\n",
    "                                        simple_value=num_steps),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"Online/Score\",\n",
    "                                        simple_value=sum(ep_rewards)),\n",
    "                                ])\n",
    "                            self.summary_writer.add_summary(performance_summary, episode_count)\n",
    "                            \n",
    "                        # TEST PERFORMANCE\n",
    "                        if episode_count % self.config['eval_freq'] == 0:  \n",
    "                            print('Evaluating at step %d' % episode_count)\n",
    "                            ep_rewards = self.test_play(sess,test_epsilon=0.01)\n",
    "\n",
    "                            performance_summary = tf.Summary(\n",
    "                                value=[\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"score/average\",\n",
    "                                        simple_value=sum(ep_rewards)/float(len(ep_rewards))),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"score/max\",\n",
    "                                        simple_value=max(ep_rewards)),\n",
    "                                    tf.Summary.Value(\n",
    "                                        tag=\"score/min\",\n",
    "                                        simple_value=min(ep_rewards)),\n",
    "                                ])\n",
    "\n",
    "                            self.summary_writer.add_summary(performance_summary, episode_count)  \n",
    "\n",
    "                        # SAVE THE MODEL\n",
    "                        if (episode_count % self.config['save_rate'] == 0 and episode_count != 0) or (episode_count == self.config['num_episodes']):\n",
    "                            self.save_model(sess,episode_count) \n",
    "                            \n",
    "                        sess.run(self.increment)                                            \n",
    "                    \n",
    "            except Exception, e:\n",
    "                # Report exceptions to the coordinator.\n",
    "                coord.request_stop(e)\n",
    "                return                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "num_actions = env.action_space.n \n",
    "env.close()\n",
    "\n",
    "Q_Network = ConvNet_Estimator(conf_parameters,num_actions,net_type=0)\n",
    "Target_Network = ConvNet_Estimator(conf_parameters,num_actions,net_type=1)\n",
    "    \n",
    "with tf.device(\"/cpu:0\"):        \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    summary_writer = tf.train.SummaryWriter(conf_parameters['log_dir'], flush_secs=30)\n",
    "\n",
    "    workers = []\n",
    "    # We are going to assign different exploration probabilities to each worker\n",
    "    epsilon_slope = 0.8 / float(num_workers-1)\n",
    "    # Create workers\n",
    "    for worker_id in range(0,num_workers):\n",
    "        worker_summary_writer = None\n",
    "        if worker_id == 0:\n",
    "            worker_summary_writer = summary_writer\n",
    "            \n",
    "        worker = Worker(\n",
    "            name=\"Worker_{}\".format(worker_id),\n",
    "            conf=conf_parameters,\n",
    "            Q_net=Q_Network,\n",
    "            Target_net=Target_Network,\n",
    "            global_episodes=global_episodes,\n",
    "            epsilon= worker_id*epsilon_slope + 0.1,\n",
    "            summary_writer = worker_summary_writer)\n",
    "            \n",
    "        workers.append(worker)\n",
    "    \n",
    "with tf.Session() as sess: \n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    # Start a thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_fn = lambda: worker.run(sess, coord)\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "\n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
