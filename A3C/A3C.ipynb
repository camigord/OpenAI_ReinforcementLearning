{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Async. one-step Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import shutil\n",
    "import threading\n",
    "import multiprocessing\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import Gym_Auxiliar as Aux_Gym\n",
    "\n",
    "from inspect import getsourcefile\n",
    "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
    "import_path = os.path.abspath(os.path.join(current_path, \"..\"))\n",
    "\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "import commonOps as cops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_parameters = {\n",
    "    'checkpoint_dir': 'Models/',\n",
    "    'log_dir': 'Logs/',\n",
    "    'env_name': 'Breakout-v0',\n",
    "    'max_global_episodes': 1000000,\n",
    "    \n",
    "    'save_rate': 5000,             # Save model after X amount of episodes\n",
    "    \n",
    "    'update_target': 1000,         # Copy parameters from training into target network\n",
    "    'async_update': 10,            # How often to perform an asynchronous update\n",
    "    \n",
    "    'eval_freq': 300,              # How often to test the performance of the network\n",
    "    'eval_episodes': 20,           # Number of episodes to run when testing\n",
    "    \n",
    "    'reset': False,                # If set, delete the existing model directory and start training from scratch.\n",
    "    'num_threads': 4,              # Number of threads to run.\n",
    "    \n",
    "    # Input size\n",
    "    'screen_width': 84,\n",
    "    'screen_height': 84,\n",
    "    'history_length': 4,\n",
    "    \n",
    "    # Gym environment\n",
    "    'pool_frame_size': 1,\n",
    "    'random_start': 30,           # Maximum number of 'do nothing' actions at the start of an episode\n",
    "    'action_repeat': 1,           # How many times should the same action be taken\n",
    "    \n",
    "    'gamma': 0.99,                # Discount factor\n",
    "    'learning_rate': 0.00025,     # Learning rate\n",
    "    \n",
    "    'min_reward': -1,\n",
    "    'max_reward': 1,\n",
    "}\n",
    "\n",
    "# Set the number of workers\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "if conf_parameters['num_threads']:\n",
    "    num_workers = conf_parameters['num_threads']\n",
    "\n",
    "# Optionally empty model directory\n",
    "if conf_parameters['reset']:\n",
    "    shutil.rmtree(conf_parameters['checkpoint_dir'], ignore_errors=True)\n",
    "    shutil.rmtree(conf_parameters['log_dir'], ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(conf_parameters['checkpoint_dir']):\n",
    "    os.makedirs(conf_parameters['checkpoint_dir'])\n",
    "    \n",
    "if not os.path.exists(conf_parameters['log_dir']):\n",
    "    os.makedirs(conf_parameters['log_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class defining global shared networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet_Estimator():\n",
    "    '''Q-value estimator neural network\n",
    "       This architecture will be used both for the Q-network and the Target network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,conf,num_actions,net_type=0):\n",
    "        if net_type == 0:\n",
    "            self.scope = 'Global_Training'\n",
    "            self.collection = 'Normal' \n",
    "        else:\n",
    "            self.scope = 'Global_Target'\n",
    "            self.collection = 'Target'\n",
    "            \n",
    "        self.num_actions = num_actions\n",
    "        self.screen_width = conf['screen_width']\n",
    "        self.screen_height = conf['screen_height']\n",
    "        self.history_length = conf['history_length']\n",
    "        self.learning_rate = conf['learning_rate']\n",
    "        self.gamma = conf['gamma']\n",
    "      \n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "                \n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        Building the network architecture\n",
    "        '''\n",
    "        self.state_ph = tf.placeholder(tf.float32,[None, self.screen_height, self.screen_width, self.history_length],name='X')\n",
    "        \n",
    "        self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.state_ph,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID', scope=\"conv1\")\n",
    "        self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID', scope=\"conv2\")\n",
    "        hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu, scope=\"FC\")\n",
    "\n",
    "        self.Q_output = slim.fully_connected(hidden,self.num_actions,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                biases_initializer=None, scope='Output')\n",
    "        \n",
    "        with tf.name_scope(\"Best_Action\"):\n",
    "            self.best_action = tf.argmax(self.Q_output,dimension=1)\n",
    "        \n",
    "        if self.scope != 'Global_Target':  \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                with tf.name_scope(\"Inputs\"):\n",
    "                    # The TD target value\n",
    "                    self.QT_ph = tf.placeholder(tf.float32,[None],name='QT_ph')\n",
    "                    # Integer id of selected action\n",
    "                    self.action_ph = tf.placeholder(tf.int32,[None],name='action_ph')\n",
    "\n",
    "                    self.reward_ph = tf.placeholder(tf.float32, [None], name=\"reward_ph\")\n",
    "                    self.terminal_ph = tf.placeholder(tf.float32, [None], name=\"terminal_ph\")\n",
    "                \n",
    "                with tf.name_scope(\"Acted_Q\"):\n",
    "                    # One hot of the action which was taken\n",
    "                    action_one_hot = tf.one_hot(self.action_ph,self.num_actions, 1., 0., name='action_one_hot')\n",
    "                    # Get the prediction of the chosen actions only\n",
    "                    acted_Q = tf.reduce_sum(self.Q_output * action_one_hot, reduction_indices=1, name='DQN_acted')\n",
    "\n",
    "                with tf.name_scope(\"Target_Q\"):\n",
    "                    Y = self.reward_ph + self.gamma * self.QT_ph * (1 - self.terminal_ph)\n",
    "                    Y = tf.stop_gradient(Y)\n",
    "\n",
    "                loss_batch = cops.clipped_l2(Y, acted_Q)\n",
    "                loss = tf.reduce_sum(loss_batch, name=\"loss\")\n",
    "        \n",
    "                self.train_op, grads = cops.graves_rmsprop_optimizer(loss, self.learning_rate, 0.95, 0.01, 1)\n",
    "        \n",
    "            with tf.name_scope(\"Summaries\"):\n",
    "                # Summaries for Tensorboard\n",
    "                self.summaries = tf.merge_summary([\n",
    "                        tf.scalar_summary('losses/loss', loss),\n",
    "                        tf.scalar_summary(\"losses/loss_max\", tf.reduce_max(loss_batch)),\n",
    "                        tf.scalar_summary('Q/avg_q',tf.reduce_mean(self.Q_output)),\n",
    "                        tf.histogram_summary('Q/q_values_hist',self.Q_output),\n",
    "                        tf.scalar_summary('Q/max_q_value',tf.reduce_max(self.Q_output)),\n",
    "                        tf.scalar_summary('Others/reward_max', tf.reduce_max(self.reward_ph))\n",
    "                        ])  \n",
    "                \n",
    "    def predict(self,sess,state):\n",
    "        '''\n",
    "        Predicts action values.\n",
    "        '''\n",
    "        return sess.run(self.Q_output,{self.state_ph:state})\n",
    "    \n",
    "    def determine_action(self,sess,state,epsilon):\n",
    "        '''\n",
    "        Predicts action based on q values for current state and exploration strategy\n",
    "        '''\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: random action\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:            \n",
    "            action = sess.run(self.best_action,{self.state_ph:[state]})[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self,sess,state,action,reward,target,terminal):\n",
    "        '''\n",
    "        Updates the estimator towards the given targets\n",
    "        '''\n",
    "        feed_dict = {self.state_ph:state, self.QT_ph:target, \n",
    "                     self.action_ph:action, self.reward_ph:reward,\n",
    "                     self.terminal_ph:terminal}\n",
    "        _, summaries = sess.run([self.train_op, self.summaries],feed_dict)\n",
    "\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "This class will allow us to stack the last K screens to use them as the input to the network (history of states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, conf):\n",
    "        self.history = np.zeros([conf['history_length'],conf['screen_height'],conf['screen_width']],dtype=np.float32)\n",
    "        \n",
    "    def add(self, screen):\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = screen\n",
    "        \n",
    "    def get(self):\n",
    "        return np.transpose(self.history,(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation worker\n",
    "\n",
    "This worker will be in charged of updating the target network, evaluating the performance of the training network and saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Monitoring_Worker():\n",
    "    '''\n",
    "    Monitors the global step, updates target network and test progress.\n",
    "    '''\n",
    "    def __init__(self, name, conf, Q_net, Target_net, global_episodes, summary_writer=None):\n",
    "        self.name = name\n",
    "        self.max_global_episodes = conf['max_global_episodes']\n",
    "        self.update_target_freq = conf['update_target']\n",
    "        self.global_Q_net = Q_net\n",
    "        self.global_Target_net = Target_net\n",
    "        self.global_episodes = global_episodes\n",
    "        self.writer = summary_writer\n",
    "        self.updated = False\n",
    "        \n",
    "        self.eval_env = Aux_Gym.GymEnvironment(conf['env_name'],conf)\n",
    "        self.eval_freq = conf['eval_freq']\n",
    "        self.eval_episodes = conf['eval_episodes']\n",
    "        self.history_length = conf['history_length']\n",
    "        \n",
    "        self.eval_history = History(conf)\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = 3)\n",
    "        self.checkpoint_dir = conf['checkpoint_dir']\n",
    "        self.save_rate = conf['save_rate']\n",
    "                \n",
    "        with tf.name_scope(\"Copy_Parameters\"):\n",
    "            self.update_target_ops = self.update_target_graph(self.global_Q_net.scope,self.global_Target_net.scope) \n",
    "        \n",
    "    def update_target_graph(self,from_scope,to_scope):\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(from_vars,to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "    \n",
    "    def play(self,sess,test_epsilon=0.01):\n",
    "        ep_rewards = []\n",
    "                \n",
    "        for episode in range(self.eval_episodes):\n",
    "            # New testing game\n",
    "            screen, terminal, score = self.eval_env.new_game(), False, 0.\n",
    "            for _ in range(self.history_length):\n",
    "                self.eval_history.add(screen)\n",
    "\n",
    "            while not terminal:\n",
    "                # Take action with exploration e= 0.01\n",
    "                action = self.global_Q_net.determine_action(sess,self.eval_history.get(),epsilon=test_epsilon)\n",
    "                # Play game in test mode (Episodes do not end when losing a life)\n",
    "                screen, reward, terminal = self.eval_env.execute_action(action,is_training=False)\n",
    "                self.eval_history.add(screen)\n",
    "                # Record every reward\n",
    "                score += reward\n",
    "\n",
    "            ep_rewards.append(score)\n",
    "        \n",
    "        return ep_rewards \n",
    "    \n",
    "    def save_model(self,sess,step=None):\n",
    "        print('Saving model at step %d' % step)\n",
    "        self.saver.save(sess,os.path.join(self.checkpoint_dir,'model'),global_step=step) \n",
    "        \n",
    "    def run(self, sess, coord):\n",
    "        print \"Starting \" + str(self.name)\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    episode_count = sess.run(self.global_episodes)\n",
    "                \n",
    "                    # Copy parameters from training to target network\n",
    "                    if episode_count % self.update_target_freq == 0:\n",
    "                        if self.updated == False:\n",
    "                            print('Updating target network')\n",
    "                            sess.run(self.update_target_ops)\n",
    "                            self.updated = True                            \n",
    "                    else:\n",
    "                        self.updated = False\n",
    "                        \n",
    "                    # Evaluate performance\n",
    "                    if episode_count % self.eval_freq == self.eval_freq - 1:  \n",
    "                        print('Evaluating at step %d' % episode_count)\n",
    "                        ep_rewards = self.play(sess,test_epsilon=0.01)\n",
    "\n",
    "                        performance_summary = tf.Summary(\n",
    "                            value=[\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"score/average\",\n",
    "                                    simple_value=sum(ep_rewards)/len(ep_rewards)),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"score/max\",\n",
    "                                    simple_value=max(ep_rewards)),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"score/min\",\n",
    "                                    simple_value=min(ep_rewards)),\n",
    "                            ])\n",
    "\n",
    "                        self.writer.add_summary(performance_summary, episode_count)  \n",
    "\n",
    "                    # Save the model\n",
    "                    if (episode_count % self.save_rate == 0 and episode_count != 0) or (episode_count == self.max_global_episodes):\n",
    "                        self.save_model(sess,episode_count) \n",
    "                      \n",
    "                    # Stop thread\n",
    "                    if episode_count >= self.max_global_episodes:\n",
    "                        print \"Finishing \" + str(self.name)\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "\n",
    "            except Exception, e:\n",
    "                # Report exceptions to the coordinator.\n",
    "                coord.request_stop(e)\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training workers\n",
    "\n",
    "Each worker will run in a different training thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,name, conf, Q_net, Target_net, global_episodes, epsilon, summary_writer=None):\n",
    "        self.name = name \n",
    "        self.max_global_episodes = conf['max_global_episodes']\n",
    "        self.global_Q_net = Q_net\n",
    "        self.global_Target_net = Target_net\n",
    "        self.global_episodes = global_episodes\n",
    "        self.summary_writer = summary_writer\n",
    "        self.async_update = conf['async_update']\n",
    "        \n",
    "        self.env = Aux_Gym.GymEnvironment(conf['env_name'],conf)\n",
    "        self.history_length = conf['history_length']        \n",
    "        self.history = History(conf) \n",
    "        \n",
    "        self.min_reward = conf['min_reward']\n",
    "        self.max_reward = conf['max_reward']\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        if self.name == 'Worker_1':\n",
    "            with tf.name_scope(\"Increment_Episode\"):\n",
    "                self.increment = self.global_episodes.assign_add(1)\n",
    "        \n",
    "    def train(self,sess,episode_buffer):\n",
    "        state, action , reward, end_state, terminal = map(np.array, zip(*episode_buffer))\n",
    "        \n",
    "        # Get the Q-value of the next state\n",
    "        q_t_plus_1 = self.global_Target_net.predict(sess,end_state)\n",
    "        # Get max Q_t+1\n",
    "        target_q = np.max(q_t_plus_1, axis=1)\n",
    "        \n",
    "        summary = self.global_Q_net.update(sess,state,action,reward,target_q,terminal)\n",
    "\n",
    "        return summary\n",
    "        \n",
    "    def run(self,sess,coord):\n",
    "        print \"Starting \" + str(self.name)\n",
    "        with sess.as_default(), sess.graph.as_default(): \n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    episode_count = sess.run(self.global_episodes)\n",
    "                    episode_buffer = []\n",
    "                    num_steps = 0\n",
    "                    ep_rewards = []\n",
    "                                        \n",
    "                    screen, terminal = self.env.new_game(), False\n",
    "\n",
    "                    for _ in range(self.history_length):\n",
    "                        self.history.add(screen)\n",
    "\n",
    "                    while not terminal:\n",
    "                        # 1. Predict: Use our training network to select an action or explore according to epsilon\n",
    "                        action = self.global_Q_net.determine_action(sess,self.history.get(),self.epsilon)\n",
    "\n",
    "                        # 2. Execute the action\n",
    "                        screen, reward, terminal = self.env.execute_action(action,is_training=True)\n",
    "                        \n",
    "                        # Compute stats to add to summaries\n",
    "                        num_steps += 1\n",
    "                        ep_rewards.append(reward)\n",
    "\n",
    "                        # Clip reward\n",
    "                        reward = max(self.min_reward,min(self.max_reward,reward))\n",
    "\n",
    "                        prev_state = self.history.get()\n",
    "                        # 3. Add screen to buffer\n",
    "                        self.history.add(screen)\n",
    "\n",
    "                        episode_buffer.append([prev_state,action,reward,self.history.get(),terminal])\n",
    "\n",
    "                        if len(episode_buffer) == self.async_update and not terminal:\n",
    "                            _ = self.train(sess,episode_buffer)\n",
    "                            episode_buffer = []\n",
    "                            \n",
    "                    # Update the network using the experience buffer at the end of the episode.\n",
    "                    if len(episode_buffer) != 0:\n",
    "                        _ = self.train(sess,episode_buffer)  \n",
    "                        \n",
    "                    if self.summary_writer is not None:\n",
    "                        performance_summary = tf.Summary(\n",
    "                            value=[\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"Online/average\",\n",
    "                                    simple_value=sum(ep_rewards)/len(ep_rewards)),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"Online/max\",\n",
    "                                    simple_value=max(ep_rewards)),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"Online/min\",\n",
    "                                    simple_value=min(ep_rewards)),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"Online/num_steps\",\n",
    "                                    simple_value=num_steps),\n",
    "                                tf.Summary.Value(\n",
    "                                    tag=\"Online/Score\",\n",
    "                                    simple_value=sum(ep_rewards)),\n",
    "                            ])\n",
    "                        self.summary_writer.add_summary(performance_summary, episode_count)\n",
    "\n",
    "                    # Stop thread\n",
    "                    if episode_count >= self.max_global_episodes:\n",
    "                        print \"Finishing \" + str(self.name)\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "                    \n",
    "                    if self.name == 'Worker_1':\n",
    "                        sess.run(self.increment)\n",
    "                    \n",
    "            except Exception, e:\n",
    "                # Report exceptions to the coordinator.\n",
    "                coord.request_stop(e)\n",
    "                return                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = Aux_Gym.GymEnvironment(conf_parameters['env_name'],conf_parameters)\n",
    "num_actions = env.env.action_space.n\n",
    "env.env.close()\n",
    "\n",
    "Q_Network = ConvNet_Estimator(conf_parameters,num_actions,net_type=0)\n",
    "Target_Network = ConvNet_Estimator(conf_parameters,num_actions,net_type=1)\n",
    "    \n",
    "with tf.device(\"/cpu:0\"):        \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    summary_writer = tf.train.SummaryWriter(conf_parameters['log_dir'], flush_secs=30)\n",
    "\n",
    "    workers = []\n",
    "    Eval_worker = Monitoring_Worker(\n",
    "            name='Worker_0',\n",
    "            conf=conf_parameters,\n",
    "            Q_net=Q_Network,\n",
    "            Target_net=Target_Network,\n",
    "            global_episodes=global_episodes,\n",
    "            summary_writer = summary_writer)\n",
    "    \n",
    "    workers.append(Eval_worker)\n",
    "    \n",
    "    # We are going to assign different exploration probabilities to each worker\n",
    "    epsilon_slope = 0.8 / float(num_workers-2)\n",
    "    # Create workers\n",
    "    for worker_id in range(1,num_workers):\n",
    "        worker_summary_writer = None\n",
    "        if worker_id == 1:\n",
    "            worker_summary_writer = summary_writer\n",
    "            \n",
    "        worker = Worker(\n",
    "            name=\"Worker_{}\".format(worker_id),\n",
    "            conf=conf_parameters,\n",
    "            Q_net=Q_Network,\n",
    "            Target_net=Target_Network,\n",
    "            global_episodes=global_episodes,\n",
    "            epsilon=(worker_id-1)*epsilon_slope + 0.1,\n",
    "            summary_writer = worker_summary_writer)\n",
    "            \n",
    "        workers.append(worker)\n",
    "    \n",
    "with tf.Session() as sess: \n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    # Start a thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_fn = lambda: worker.run(sess, coord)\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "\n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Neptuno]",
   "language": "python",
   "name": "Python [Neptuno]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
